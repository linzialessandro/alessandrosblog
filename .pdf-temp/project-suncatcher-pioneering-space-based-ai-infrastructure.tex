\documentclass[11pt,a4paper,oneside]{memoir}

% Typography + fonts
\usepackage{fontspec}
\usepackage[T1]{fontenc}
\usepackage{microtype}

% Font fallback: use Libertinus if available, otherwise TeX Gyre Pagella.
\IfFontExistsTF{Libertinus Serif}{
  \setmainfont{Libertinus Serif}
  \setsansfont{Libertinus Sans}
  \setmonofont{Libertinus Mono}
}{
  \setmainfont{TeX Gyre Pagella}
  \setsansfont{TeX Gyre Heros}
  \setmonofont{TeX Gyre Cursor}
}

% Layout (memoir-native)
\setlrmarginsandblock{1.15in}{1.15in}{*}
\setulmarginsandblock{1.0in}{1.0in}{*}
\checkandfixthelayout

% No headers/footers/page numbers
\pagestyle{empty}

% Paragraph style
\setlength{\parindent}{0pt}
\setlength{\parskip}{0.55\baselineskip}

% Pandoc helpers
\usepackage{xcolor}
\usepackage[hidelinks]{hyperref}
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\begin{document}

{\Large\bfseries Project Suncatcher: Pioneering Space-Based AI Infrastructure\par}
\vspace{1.25em}

A question keeps coming up as AI scales: where do we find the energy and infrastructure to keep pushing compute forward without squeezing Earth's resources even harder? Project Suncatcher is one of the boldest answers on the table---a research moonshot that explores whether serious machine learning compute could eventually live in orbit, powered directly by near-continuous sunlight.

\subsection{Harnessing Solar Power for AI}\label{harnessing-solar-power-for-ai}

The basic premise is straightforward: the Sun delivers an absurd amount of energy, and in the right orbit satellites can stay in sunlight for most (or nearly all) of their operational time. Project Suncatcher imagines compact constellations of solar-powered satellites, equipped with TPUs, designed to behave like a distributed ``data center'' in space while relying far less on terrestrial constraints like land and water.

\subsection{Overcoming Technical Challenges}\label{overcoming-technical-challenges}

Scaling compute in space isn't about launching one big box---it's about making many satellites act like one cohesive system. The project highlights several core challenges that need to be solved before this idea can move from speculative to practical:

\begin{enumerate}
\tightlist
\item
  High-bandwidth inter-satellite links: To run distributed ML workloads, the satellites would need data center-scale connectivity, targeting tens of terabits per second per link using approaches like dense wavelength-division multiplexing (DWDM) and spatial multiplexing.
\item
  Satellite formation control: Achieving those optical link budgets requires satellites flying in relatively tight and stable formations, with models that account for gravitational and orbital dynamics.
\item
  Radiation tolerance: Compute hardware in orbit must handle radiation exposure; the work discusses testing that suggests TPU designs can be more resilient than many people assume.
\item
  Economic viability: Even if the engineering works, launch costs have to fall far enough for orbital compute to compete with terrestrial buildouts, with projections pointing to improvements by the mid-2030s.
\end{enumerate}

\subsection{Looking Ahead}\label{looking-ahead}

What makes this feel more than a thought experiment is the plan to validate pieces of the stack in orbit. A learning mission in partnership with Planet is planned to launch prototype satellites by early 2027 to test hardware and key assumptions in the real environment.

The reason Project Suncatcher is worth paying attention to is not because ``AI data centers in space'' is guaranteed to happen, but because it forces the right systems-level conversation: energy, networking, reliability, and the physics of scaling. If the next decade is about expanding compute responsibly, exploring extreme options like this helps map the boundary of what's possible.


\end{document}
