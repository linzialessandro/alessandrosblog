{
  "posts": [
    {
      "title": "2026 will be the year AI stops being “a tool” and starts being infrastructure",
      "slug": "2026-ai-stops-being-a-tool-and-becomes-infrastructure",
      "publishedAt": "2026-01-05T10:17:00+01:00",
      "summary": "Stanford’s HAI roundup of expert predictions for 2026 reads like a set of pressure points: trust, transparency, and the awkward fact that deployment decisions are now social decisions. The interesting part is not the forecasts themselves, but what they imply about how we should build, buy, and use AI systems this year.",
      "tags": [
        "AI",
        "Policy",
        "Society"
      ],
      "content": "<p>Predictions are usually entertainment: a safe way to sound informed without committing to anything testable.</p><p>But once AI becomes something people rely on for work, school, and decisions that affect them, forecasts stop being trivia. They become a way to name the failure modes early, while there is still time to design around them.</p><p>Stanford HAI’s “AI experts predict what will happen in 2026” is useful in that pragmatic sense. It frames the coming year less as a sequence of shiny releases and more as a set of tensions that will show up in ordinary life: who trusts what, what counts as evidence, what gets automated (and what shouldn’t), and how much opacity society is willing to tolerate.</p><h2>The real story: trust is now a product requirement</h2><p>When people ask whether a therapy chatbot can be trusted, that is not a niche “AI safety” question. It’s a demand for reliability in a context where mistakes are emotionally costly.</p><p>When employees worry their boss is automating the wrong job, they are not resisting technology. They are pointing out that automation has second-order effects: it reshapes incentives, desksills teams, and changes what gets measured.</p><p>And when users worry their private conversations are training tomorrow’s models, they are implicitly asking for enforceable boundaries, not marketing language.</p><p>In other words, 2026 is not primarily about capability. It’s about trustworthiness becoming legible and auditable enough that non-experts can reason about it.</p><h2>Transparency is becoming a competitive axis</h2><p>There’s a weird inversion happening. As models get stronger, public visibility into how they are built and evaluated can get weaker.</p><p>This is partly business reality: the incentives of competition push toward secrecy. But it creates a governance gap, because you can’t meaningfully assess risk without basic information about training, evaluation, and deployment constraints.</p><p>In 2026, “transparent enough” will matter across multiple layers:</p><ul><li><strong>For users</strong>: what data is used, what is retained, and what choices exist.</li><li><strong>For organizations</strong>: what gets logged, what can be audited, and what can be turned off.</li><li><strong>For regulators and the public</strong>: what claims are supported by evidence rather than vibes.</li></ul><p>The bad equilibrium is predictable: opacity rises, trust erodes, then policy arrives as a blunt instrument. The better equilibrium is also clear: transparency becomes a feature, and systems that can prove what they do earn adoption.</p><h2>Automation will be judged by what it breaks</h2><p>Most automation narratives focus on what becomes faster. The more honest question is what becomes fragile.</p><p>Automating the wrong work can remove exactly the parts of a job that keep the system safe: informal checks, human intuition for edge cases, and the quiet responsibility people feel when they own an outcome.</p><p>A useful way to think about 2026 is that automation will move from “can we do it?” to “what does it do to the surrounding system?” That includes workplaces, education, and even personal relationships with information.</p><h2>Privacy is no longer a preference, it’s a boundary condition</h2><p>The old framing was: privacy is a personal value, so users can trade it for convenience.</p><p>The new framing is: privacy is the condition that makes some uses acceptable at all. If conversations that feel private are later repurposed into training data, adoption becomes socially unstable. People will self-censor, avoid sensitive use cases, and treat AI systems as untrustworthy witnesses.</p><p>So the practical question for 2026 is not “does the model work?” It is “does the system have clear, enforceable data boundaries, and can those boundaries survive normal operational pressure?”</p><h2>What to do with these predictions (if you build or teach)</h2><p>Predictions are only useful if they change behavior. A few concrete moves that follow from the pressures Stanford highlights:</p><ul><li><strong>Prefer systems with controllable data flows</strong>: retention settings, clear opt-ins, and the ability to separate sensitive work from general usage.</li><li><strong>Instrument verification</strong>: require sources, logs, and traces where appropriate, and make “I don’t know” an acceptable output.</li><li><strong>Design for misuse and misunderstanding</strong>: assume users will over-trust confident language, and build guardrails that reduce that harm.</li><li><strong>Teach epistemic hygiene</strong>: not prompt tricks, but habits of checking, comparing, and stating uncertainty.</li></ul><p>If 2025 was the year people learned models could do impressive things, 2026 looks like the year society learns the uncomfortable part: impressive is cheap, but trustworthy is engineered.</p><p><strong>Contributor:</strong> Alessandro Linzip</p><p>Read more here: <a href=\"https://hai.stanford.edu/news/stanford-ai-experts-predict-what-will-happen-in-2026\" target=\"_blank\" rel=\"noopener noreferrer\">Stanford AI Experts Predict What Will Happen in 2026</a></p>"
    },
    {
      "title": "NotebookLM Deep Research is what “efficient research” should feel like",
      "slug": "notebooklm-deep-research-efficient-ai-workflow",
      "publishedAt": "2026-01-04T12:03:00+01:00",
      "summary": "NotebookLM’s new Deep Research feature treats sources as the workflow, not a footnote. It builds a source-grounded report by browsing hundreds of sites, then lets you import the report and sources into your notebook so you can keep thinking without context switching.",
      "tags": [
        "AI",
        "Productivity",
        "Google"
      ],
      "content": "<p>Efficiency in AI workflows is rarely about the model being “smart”. It’s about whether your context stays intact long enough to matter.</p><p>Most people don’t lose time because they can’t find information. They lose time because the research loop shatters into fragments: search results in one place, notes in another, a half-written draft somewhere else, and a model chat that forgets what you did five minutes ago.</p><p>Google’s latest NotebookLM update is worth paying attention to because it’s a very opinionated answer to that fragmentation: build a rich set of sources, keep them attached to the work, and make the AI operate inside that boundary.</p><h2>Deep Research is “source acquisition” as a feature</h2><p>Deep Research is positioned as a research agent: you give it a question, it creates a research plan, browses hundreds of websites, refines its search as it learns, and produces an organized, source-grounded report.</p><p>The subtle detail is that the report is not treated as a final deliverable. In NotebookLM, the report is an intermediate artifact you can pull into your notebook and then keep interrogating.</p><p>This is the workflow shift: instead of asking the model to be correct in one shot, you ask the system to assemble a corpus and then you do iterative thinking on top of it.</p><h2>The report is just the beginning</h2><p>NotebookLM explicitly frames Deep Research as a bootstrap mechanism: you can add the report and its sources into the notebook, then continue working with the rest of your materials while Deep Research runs in the background.</p><p>That matters because “research” is rarely linear. The moment you read something interesting, you want to pull in adjacent sources, compare claims, extract definitions, and rewrite your mental model. The faster you can do that without leaving the workspace, the more likely you are to stay in a productive state.</p><p>In practice, this also changes how prompts should be written. Instead of prompting for answers, prompt for corpus-building: what sources are missing, which concepts need multiple viewpoints, what is likely to be misunderstood, what should be verified, and what deserves an explicit counterexample.</p><h2>Why this makes an AI workflow efficient</h2><p>There’s a predictable pattern in knowledge work:</p><ul><li>At the beginning you need breadth (coverage, vocabulary, map of the territory).</li><li>Then you need structure (a plan, a taxonomy, a set of subquestions).</li><li>Finally you need compression (a brief, a lesson, a decision memo, a publishable draft).</li></ul><p>Deep Research is a direct attempt to speed up the breadth phase without losing traceability, because the output is grounded in sources you can keep attached to the notebook.</p><p>It also reduces the worst productivity killer: repeated “reloading” of context. If a tool forces you to restate what you’ve read and why it matters every time you switch steps, you spend your day paying a tax on your own memory.</p><h2>More source types means fewer excuses</h2><p>The second half of the update is less flashy but arguably more important: NotebookLM is expanding the file types you can treat as sources.</p><p>Google highlights support for Google Sheets (structured data), Drive files as URLs (copy-paste like a normal link), images (including photos of handwritten notes), PDFs directly from Google Drive, and Microsoft Word documents (.docx).</p><p>This matters because real projects are messy. Research is not only “articles and papers”; it’s also spreadsheets, drafts, screenshots, and notes captured at the wrong time in the wrong place. The more friction there is in turning those artifacts into a coherent source set, the more likely you are to abandon the attempt and go back to ad‑hoc prompting.</p><h2>A concrete workflow that actually holds up</h2><p>Here’s a workflow that makes Deep Research useful rather than just impressive:</p><ul><li><strong>Start with Deep Research</strong> to generate a first corpus and a report, then import both into the notebook.</li><li><strong>Interrogate the corpus</strong>: extract key terms, competing definitions, and claims that appear “obvious” but need checking.</li><li><strong>Force disagreement</strong>: ask for the strongest counterarguments and for where sources conflict, not just where they align.</li><li><strong>Build an outline</strong> that separates facts (source-backed) from interpretations (yours) and from open questions (explicitly unknown).</li><li><strong>Write last</strong>: treat writing as selection and synthesis, not as exploration.</li></ul><p>This turns NotebookLM into something closer to an instrumented research environment: sources go in, questions get sharper, and the output becomes a byproduct of a stable knowledge base.</p><p>The risk is also straightforward: speed makes it tempting to stop early. A “source-grounded” report can still be shallow, cherry-picked, or mismatched to your actual goal. The point of the notebook is that it makes ongoing verification cheap enough that you might actually do it.</p><p><strong>Contributor:</strong> Alessandro Linzi</p><p>Read more here: <a href=\"https://blog.google/technology/google-labs/notebooklm-deep-research-file-types/\" target=\"_blank\" rel=\"noopener noreferrer\">Google Blog</a></p>"
    },
    {
      "title": "Cyber resilience won’t come from one safeguard",
      "slug": "cyber-resilience-not-one-safeguard",
      "publishedAt": "2026-01-02T19:00:00+01:00",
      "summary": "As AI gets better at cybersecurity tasks, the uncomfortable truth is that the same capability can help defenders and attackers. OpenAI’s latest stance is to design for defenders first and treat safety as a layered, evolving system—not a single gate you lock once.",
      "tags": [
        "AI",
        "Cybersecurity",
        "Safety",
        "OpenAI"
      ],
      "content": "<p>Cybersecurity has always been an arms race, but AI changes the tempo. When models start performing well on real security tasks, they don’t just speed up triage and patching; they also risk lowering the cost of offensive work if deployed carelessly.</p><p>What I found notable in OpenAI’s “Strengthening cyber resilience” framing is that it doesn’t pretend you can solve this with a single policy switch. The claim is closer to an engineering principle: if the domain is inherently dual-use, then the response has to be defense-in-depth, continuously updated, and anchored to real-world defender workflows.</p><h2>Why “defenders first” is a real constraint</h2><p>In security, “build for defenders” is not a slogan; it’s an allocation decision. It means prioritizing capabilities that make audits faster, vulnerability discovery more systematic, and incident response less chaotic—especially in teams that are under-resourced compared to attackers.</p><p>Third-party reporting summarizing OpenAI’s position emphasizes defensive tooling as the priority as advanced systems expand, with investments aimed at helping teams audit code, patch vulnerabilities, and respond more effectively to threats. That is the right direction, because it treats the bottleneck as operational capacity, not just model intelligence.</p><h2>Defense-in-depth for models (not just networks)</h2><p>The old mental model in cybersecurity is: isolate, authenticate, log, and monitor. The updated model for frontier AI looks similar, but the layers move closer to the model and its deployment surface: access controls, monitoring, detection, and red teaming, all treated as a combined stack rather than independent checkboxes.</p><p>Again, the same summary highlights the argument that cybersecurity can’t be governed through a single safeguard because defensive and offensive techniques overlap; instead, the approach described is defense-in-depth combining access controls, monitoring, detection systems, and extensive red teaming.</p><h2>Ecosystem work matters more than a product launch</h2><p>One underappreciated point: “safety” isn’t just a model property, it’s an ecosystem property. If a lab is serious, it will build programs and institutions that create feedback loops with practitioners who see real attacks, not toy examples.</p><p>The same report notes planned initiatives like trusted access programs for defenders, agent-based security tools in private testing, and the creation of a Frontier Risk Council—signals that the plan is long-term governance plus practical deployment scaffolding, not just model-side filtering.</p><h2>A practical takeaway for teams</h2><p>If deploying AI in a security-sensitive environment, the useful question is not “Is the model safe?” but “What layered controls exist around its use, and how quickly do they adapt?”</p><ul><li>Require identity-bound access and role separation for high-impact features (don’t let “helpful” become “powerful-by-default”).</li><li>Log prompts/outputs with retention rules, then actually review samples (monitoring without review is theater).</li><li>Red-team your internal workflows, not just the model (the exploit path is usually socio-technical).</li></ul><p><strong>Contributor:</strong> Alessandro Linzi</p><p>Read more here: <a href=\"https://openai.com/index/strengthening-cyber-resilience/\" target=\"_blank\" rel=\"noopener noreferrer\">OpenAI: Strengthening cyber resilience as AI capabilities advance</a></p>"
    },
    {
      "title": "2025: The Year Reasoning and Agents Became Real",
      "slug": "2025-year-in-llms-reasoning-agents",
      "publishedAt": "2026-01-01T10:00:00+01:00",
      "summary": "Reflecting on 2025, the landscape of LLMs shifted decisively toward reasoning and agentic workflows. Simon Willison's annual review highlights how Reinforcement Learning from Verifiable Rewards (RLVR) and CLI-based coding agents have transformed models from chatbots into problem solvers capable of rigorous tasks.",
      "tags": [
        "AI",
        "Reasoning",
        "Agents",
        "Math",
        "Coding"
      ],
      "content": "<p>2025 marked the moment LLMs stopped just predicting the next token and started <em>thinking</em>. Simon Willison’s comprehensive review of the year identifies \"reasoning\" as the defining trend, driven by a technique known as Reinforcement Learning from Verifiable Rewards (RLVR). This shift has fundamentally changed how we interact with these systems, moving us from chat interfaces to asynchronous agentic workflows.</p><h2>The \"Reasoning\" Unlock: RLVR</h2><p>For a mathematician, the shift to RLVR is the most significant architectural change. By training models against automatically verifiable environments—such as math puzzles or code execution—labs like OpenAI (with o1/o3) and DeepSeek (with R1) taught models to develop problem-solving strategies. As Andrej Karpathy noted, these models learn to break down problems and \"backtrack\" when they hit a dead end. This isn't just about better conversation; it's about allocating compute to <em>inference time</em> rather than just pre-training. Willison points out that this capability is what finally made AI-assisted search and complex debugging reliable: the model can reason about its own errors.</p><h2>The Era of Coding Agents and \"YOLO Mode\"</h2><p>The practical application of this reasoning is the \"coding agent.\" The release of Claude Code defined the year, moving developers away from copy-pasting code snippets to using <strong>asynchronous agents</strong> that run in the background. These tools can plan multi-step tasks, execute code, inspect the results, and iterate. However, Willison highlights a critical security trade-off: the \"Normalization of Deviance.\" To make these agents useful, developers are increasingly running them in \"YOLO mode\" (bypassing approval prompts), granting autonomous AI systems read/write access to their local environments—a practice that feels efficient until it inevitably isn't.</p><h2>Conformance Suites as Prompts</h2><p>Perhaps the most exciting development for those of us who value formal rigor is the rise of <strong>conformance suites</strong>. Willison observed that the most effective way to drive a 2025-era agent isn't a text prompt, but a rigorous test suite. If you provide a model with a comprehensive set of tests (like the html5lib tests or a WebAssembly spec), it can iterate independently until the implementation passes. This aligns perfectly with formal verification methods: the \"spec\" becomes the instruction, and the agent becomes the implementation engine.</p><h2>The Open Weight Geopolitics</h2><p>Finally, the monopoly on intelligence fractured in 2025. Chinese labs released open-weight models (like DeepSeek V3 and Qwen 2.5) that didn't just catch up to US models—they occasionally beat them, causing temporary panic in US markets (specifically impacting NVIDIA). Alongside Google's custom TPU-driven Gemini ecosystem, this has created a diverse, highly competitive field where \"state-of-the-art\" changes monthly.</p><p><strong>Contributor:</strong> Alessandro Linzi</p><p>Read more here: <a href=\"https://simonwillison.net/2025/Dec/31/the-year-in-llms/\" target=\"_blank\" rel=\"noopener noreferrer\">Simon Willison's Weblog</a></p>"
    },
    {
      "title": "Zhipu AI: The First AI Company to Go Public",
      "slug": "zhipu-ai-first-ai-ipo",
      "publishedAt": "2025-12-30T11:48:00+01:00",
      "summary": "Zhipu AI, officially known as Knowledge Atlas Technology, has priced its Hong Kong IPO and will begin trading on January 8, 2026, becoming the first major large language model company to go public globally.",
      "tags": [
        "AI",
        "IPO",
        "China"
      ],
      "content": "<p>Zhipu AI, officially registered as Knowledge Atlas Technology Joint Stock Co. Ltd., has priced its Hong Kong IPO, positioning itself to become the first major large language model (LLM) company to list on a global exchange. The Beijing-based startup, which emerged from Tsinghua University in 2019, finalized the issuance of 37.42 million new H-shares at HK$116.20 each, raising approximately HK$4.35 billion (around $560 million). Trading is scheduled to begin on January 8, 2026, under ticker symbol 2513.</p><h2>Valuation and Investors</h2><p>The IPO values Zhipu at approximately HK$51.8 billion ($6.7 billion). Before going public, the company secured significant capital through funding rounds backed by major tech players including Alibaba, Tencent, Ant Group, and Saudi Aramco. Cornerstone investors have already committed roughly HK$3 billion to the offering, demonstrating strong institutional confidence.</p><h2>Business and Performance</h2><p>Known as one of China's \"AI Six Tigers,\" Zhipu has established itself through both open-source contributions and commercial services. The company plans to allocate 70% of IPO proceeds to R&D for general-purpose large AI models and 10% toward optimizing its Model-as-a-Service (MaaS) platform. Revenue grew from modest levels to 191 million yuan in the first half of 2025, though the company reported a net loss of 2.36 billion yuan during the same period, primarily due to high computing power expenses.</p><h2>Global First</h2><p>Zhipu's listing beats both Western competitors like OpenAI and Anthropic, as well as domestic rival Minimax, to become the world's first publicly traded foundation model company. This milestone comes at a time when the AI industry faces intense pressure to demonstrate viable business models alongside cutting-edge capabilities. The company internationally markets itself as Z.ai and has gained recognition for its GLM model series.</p><p><strong>Contributor:</strong> Alessandro Linzi</p><p>Read more here: <a href=\"https://pandaily.com/zhipu-ai-launches-hong-kong-ipo-with-hk-3-billion-in-cornerstone-commitments-poised-to-be-2026-s-largest-opening-ipo/\" target=\"_blank\" rel=\"noopener noreferrer\">Pandaily</a></p>"
    },
    {
      "title": "Math Fine-Tuning in 2025: SFT is Dead, Long Live GRPO",
      "slug": "math-fine-tuning-2025-grpo-unsloth",
      "publishedAt": "2025-12-28T16:00:00+01:00",
      "summary": "Standard Supervised Fine-Tuning (SFT) works for style, but for mathematical reasoning, it hits a ceiling. The new state of the art for local models is incentivizing reasoning traces via Group Relative Policy Optimization (GRPO), allowing 7B models to self-correct without a massive value network.",
      "tags": [
        "Unsloth",
        "Math AI",
        "DeepSeek",
        "Local LLM"
      ],
      "content": "<p>If you are still fine-tuning 7B models on math datasets using standard Supervised Fine-Tuning (SFT), you are essentially teaching the model to <em>mimic</em> the appearance of a solution rather than the act of solving it. The signal from 2024/2025 is clear: <strong>inference-time compute</strong> is not just a prompting trick; it is a training objective.</p>\n\n<p>The breakthrough for local labs is that we no longer need the massive infrastructure required for PPO (Proximal Policy Optimization). We have moved to <strong>GRPO (Group Relative Policy Optimization)</strong>, which removes the need for a critic model and fits reasoning training onto a single consumer GPU.</p>\n\n<h2>The Shift: From Imitation (SFT) to Incentive (GRPO)</h2>\n<p>In SFT, the model learns to predict the next token in a \"Chain of Thought\" (CoT) trace. The problem? If the trace contains a logical jump or a hallucination, the model memorizes the error just as firmly as the truth.</p>\n\n<p>GRPO changes the game by sampling a <em>group</em> of outputs for the same prompt and optimizing based on a reward function (correctness of the final answer). The policy is updated to maximize the probability of outputs that score higher than the group average.</p>\n\n<p>Crucially, the baseline is computed from the group mean, not a separate value network. This cuts VRAM usage nearly in half, making it possible to train reasoning behaviors on a 24GB card.</p>\n\n<h2>The Stack: Unsloth + Qwen-2.5-Math</h2>\n<p>The most viable path for a home lab right now involves <strong>Unsloth</strong>. They have integrated GRPO directly into their training pipeline. You don't need a complex reward model for math; you just need a deterministic way to check the final answer.</p>\n\n<p>Here is the stripped-down logic for a local GRPO run:</p>\n\n<pre><code class=\"language-python\">from unsloth import FastLanguageModel, PatchFastRL\nPatchFastRL(\"GRPO\", FastLanguageModel)\n\n# 1. Load a strong base model (Qwen-2.5-Math-7B is excellent)\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"Qwen/Qwen2.5-Math-7B\",\n    load_in_4bit = True\n)\n\n# 2. Define a verifiable reward function\ndef reward_correctness(prompts, completions, answer, **kwargs):\n    # Extract answer from completion and match with ground truth\n    return [1.0 if extract_answer(c) == a else 0.0 for c, a in zip(completions, answer)]\n\n# 3. Train with Group Sampling (GRPO)\ntrainer = GRPOTrainer(\n    model = model,\n    reward_funcs = [reward_correctness],\n    args = GRPOConfig(\n        per_device_train_batch_size = 1, \n        gradient_accumulation_steps = 4,\n        num_generations = 4, # Group size\n        max_prompt_length = 512,\n        max_completion_length = 1024,\n    ),\n)\ntrainer.train()</code></pre>\n\n<h2>Why this matters for Mathematicians</h2>\n<p>This approach allows us to train models that <strong>self-correct</strong>. By rewarding only the final answer, we allow the model to \"learn\" that spending more tokens to double-check its work (or backtrack) leads to a higher reward. You aren't forcing a specific proof style; you are incentivizing the <em>process</em> of being right.</p>\n\n<p><strong>Contributor:</strong> Alessandro Linzi</p>\n\n<p>Read more here: <a href=\"https://unsloth.ai/blog/r1-reasoning\" target=\"_blank\" rel=\"noopener noreferrer\">Unsloth AI</a></p>"
    },
    {
      "title": "AI for Mathematical Discovery: From Guessing to Verifying",
      "slug": "ai-mathematical-discovery-from-guessing-to-verifying",
      "publishedAt": "2025-12-27T17:00:00+01:00",
      "summary": "AI is starting to matter in math not because it “knows” theorems, but because it can search, propose, and—crucially—verify. The interesting shift is the coupling of large generative models with formal proof checkers and symbolic engines, turning mathematical discovery into a loop of conjecture, proof search, and certified correctness.",
      "tags": [
        "AI",
        "Mathematics",
        "Theorem Proving",
        "DeepMind"
      ],
      "content": "<p>There’s a specific kind of optimism that shows up whenever AI touches mathematics: the dream that a machine will stumble onto a new theorem the way a human does—by taste, by pattern, by obsession. What’s actually happening is less romantic and (to me) more interesting: we are learning how to make discovery <em>operational</em>.</p>\n\n<p>“Operational” here means a loop you can run: translate a problem into a formal object, generate candidate steps, check them, and feed the verified signal back into the system. Not a single shot “answer”, but a process that can grind for hours, days, weeks—without drifting into hallucination.</p>\n\n<h2>Why formal systems change the game</h2>\n<p>If you’ve used Isabelle/HOL, Lean, Coq, or friends, you already know the key point: a proof assistant is not impressed by eloquence. It accepts only a proof that type-checks and reduces where it should. That strictness is exactly what current language models lack in natural language mode: they can be persuasive while being wrong.</p>\n\n<p>The deep idea in many recent systems is to exploit a verifier as the ultimate critic. The model is allowed to be creative and messy, but every step gets forced through a tiny gate: does the checker accept it?</p>\n\n<h2>DeepMind’s IMO result as a case study</h2>\n<p>DeepMind described a system that reached silver-medal level on the 2024 International Mathematical Olympiad by combining two complementary components: AlphaProof (focused on formal reasoning in Lean via reinforcement learning) and AlphaGeometry 2 (a neuro-symbolic geometry solver with a fast symbolic engine). That pairing matters: one part searches proofs in a formal language, the other has domain-specific geometry machinery.</p>\n\n<p>Two details are worth noticing. First, the IMO problems were manually translated into a formal language before the systems could work on them, which highlights that “autoformalization” is still a major bottleneck. Second, once in the formal domain, the systems could spend serious compute time exploring proof space, and the correctness of any found proof is not a vibe—it is mechanically verified.</p>\n\n<h2>The real frontier: conjectures, not just solutions</h2>\n<p>Solving a fixed problem is already hard, but discovery is often about proposing the right intermediate statements. In practice, mathematicians don’t move linearly from axioms to theorem; they invent lemmas, strengthen hypotheses, and adjust definitions until the landscape becomes navigable.</p>\n\n<p>AI systems that live inside a formal environment (Lean, Isabelle, etc.) can in principle search not only for proofs but for useful stepping stones—lemmas that shorten proofs, reusable tactics, or alternative formulations that make automation feasible. That’s where “math discovery” starts to feel real: not replacing insight, but mass-producing plausible local moves and letting the verifier keep only the ones that survive.</p>\n\n<h2>What this suggests for working mathematicians</h2>\n<ul>\n<li><strong>Verification becomes the backbone</strong>: any workflow that keeps the model tethered to a proof checker avoids the most damaging failure mode (confident nonsense).</li>\n<li><strong>Human time shifts</strong>: less time spent on low-level bookkeeping and more time spent choosing definitions, deciding which subproblems matter, and interpreting what a formal proof is actually saying.</li>\n<li><strong>Autoformalization is strategic</strong>: the moment “informal → formal” becomes reliable, the space of problems accessible to these loops expands dramatically.</li>\n</ul>\n\n<p>None of this guarantees new Fields-level theorems in the short term. But it does point to a plausible medium-term reality: mathematicians using AI the way programmers use compilers and fuzzers—tools that don’t supply meaning, but relentlessly enforce correctness while exploring huge spaces of possibilities.</p>\n\n<p><strong>Contributor:</strong> Alessandro Linzi</p>\n\n<p>Read more here: <a href=\"https://deepmind.google/blog/ai-solves-imo-problems-at-silver-medal-level/\" target=\"_blank\" rel=\"noopener noreferrer\">DeepMind Blog</a></p>"
    },
    {
      "title": "Agentic Engineering: Simplicity is the Ultimate Sophistication",
      "slug": "agentic-engineering-simplicity-just-talk-to-it",
      "publishedAt": "2025-12-26T15:00:00+01:00",
      "summary": "Peter Steinberger argues that the most effective agentic workflow isn't a complex framework, but a direct conversation with a capable model. As context windows grow and models improve, the best tooling might just be a CLI and good intuition.",
      "tags": [
        "AI",
        "Agents",
        "Productivity"
      ],
      "content": "<p>There is a tendency to over-engineer solutions before we fully understand the problem. In the exploding world of AI agents, this manifests as complex harnesses, RAG pipelines for codebases, and elaborate \"sub-agent\" architectures. Peter Steinberger’s latest post, <em>Just Talk To It</em>, offers a refreshing counter-argument: the best agentic workflow is often just a direct conversation.</p><h2>The \"Blast Radius\" heuristic</h2><p>One of the most useful mental models Steinberger introduces is the concept of <strong>\"blast radius\"</strong>. Before asking an agent to make changes, he assesses the potential impact: is this a small, atomic commit (a \"small bomb\") or a massive refactor (a \"Fat Man\")?</p><p>This is engineering intuition applied to AI. You don't need a formal \"plan mode\" for every tweak, but you do need to know when to stop, check the status, or ask for options before proceeding. It turns the interaction from a \"prompt and pray\" loop into a collaborative steering process.</p><h2>Collapsing the complexity</h2><p>The post argues that many tools (MCPs, heavy frameworks) are solving problems that are rapidly disappearing. With effective context windows reaching ~230k tokens and models like GPT-5-codex, the need for RAG over a codebase diminishes. The model can simply read the relevant files.</p><p>Steinberger suggests that we are moving past the phase of needing complex scaffolding to make models useful. Instead, the tooling should fade into the background—simple CLIs, fast terminals, and atomic git operations—letting the engineer focus on the high-level architecture and the conversation with the model.</p><h2>The model as a senior engineer</h2><p>I appreciate the shift in perspective here: treating the AI not as a function call, but as a senior engineer. You delegate, you review, and sometimes you just \"talk to it\" to flesh out an idea. As Steinberger puts it, \"Just because I don’t write the code anymore doesn’t mean I don’t think hard about architecture.\"</p><p><strong>Contributor:</strong> Alessandro Linzi</p><p>Read more here: <a href=\"https://steipete.me/posts/just-talk-to-it\" target=\"_blank\" rel=\"noopener noreferrer\">Peter Steinberger: Just Talk To It</a></p>"
    },
    {
      "title": "Command Line Interface Guidelines: A modern philosophy for the terminal",
      "slug": "command-line-interface-guidelines-modern-philosophy",
      "publishedAt": "2025-12-25T12:00:00+01:00",
      "summary": "The Command Line Interface Guidelines (CLIG) is an open-source manifesto for writing better CLI programs. It shifts the focus from 'machine-first' UNIX traditions to 'human-first' design, emphasizing that a terminal tool is a conversational interface that requires empathy, discoverability, and robust feedback loops.",
      "tags": [
        "CLI",
        "Open Source",
        "Best Practices"
      ],
      "content": "<p>We often treat command-line tools as pure utility: scripts to be piped, args to be parsed, and exit codes to be checked. But as computing pioneer Alan Kay noted, the command line remains the most <em>versatile</em> corner of the computer, offering a depth GUIs cannot afford. The Command Line Interface Guidelines (CLIG) proposes a necessary modernization: shifting from a <strong>machine-first</strong> tradition to a <strong>human-first</strong> design philosophy.</p><h2>The Baggage of the Past</h2><p>Traditional UNIX philosophy optimized for composability and brevity—machines talking to machines. This often resulted in opaque error messages and the silent treatment on success (the classic &quot;no news is good news&quot; approach of <code>cp</code>). CLIG argues that today’s terminal is primarily a human workspace. If a command changes the state of the system, it should explicitly tell the user what happened, much like <code>git push</code> details exactly which refs were updated, rather than just returning exit code 0.</p><h2>Conversation as the Interface</h2><p>The guide introduces a powerful metaphor: <em>conversation</em>. Using a CLI is a dialogue where the user proposes an intent, and the program responds. This reframes error handling not as a crash, but as a corrective turn in the conversation. </p><p>For instance, if I run <code>brew update jq</code>, the tool shouldn&#39;t just fail; it should gently suggest, <em>&quot;Did you mean `brew upgrade jq`?&quot;</em> This is empathy in software design. It acknowledges that the user is learning through repeated trial-and-error.</p><h2>Concrete Heuristics for Modern Tools</h2><p>The document is rich with specific implementation details that distinguish a &quot;toy&quot; script from a professional tool:</p><ul><li><strong>Context-Aware Output (The TTY Rule)</strong>: Your program must know its audience. If <code>stdout</code> is an interactive terminal, give me colors, spinners, and human-readable tables. If it’s being piped to a file or another command, strip the ANSI codes and animations immediately. A progress bar in a CI log isn&#39;t a feature; it&#39;s a corrupt log file.</li><li><strong>The &quot;Help&quot; Hierarchy</strong>: Documentation isn&#39;t an afterthought. CLIG suggests a tiered approach: running the command bare should show concise usage examples (like <code>jq</code>), while <code>--help</code> should provide the full manual. Crucially, lead with <strong>examples</strong>, not syntax definitions. Users learn by pattern-matching, not by reading Backus–Naur form.</li><li><strong>Flags over Arguments</strong>: Positional arguments are brittle relics. While <code>cp source dest</code> is standard, it is also ambiguous. Modern tools should prefer flags (<code>--file</code>, <code>--output</code>) which are explicit, order-independent, and extensible without breaking backward compatibility.</li><li><strong>JSON as a First-Class Citizen</strong>: Since we operate in a web-centric world, tools should support a <code>--json</code> flag. This allows users to bypass text parsing tools like <code>awk</code> and pipe structured data directly into <code>jq</code> or networked services.</li></ul><h2>Why this matters</h2><p>I value this guide because it treats CLI design with the same rigor we apply to graphical interfaces. It recognizes that <strong>robustness</strong> is a subjective feeling—a tool that catches interrupts (<code>Ctrl-C</code>) gracefully and provides feedback in under 100ms feels &quot;solid,&quot; regardless of its actual execution time. A well-designed CLI respects my time by being discoverable and respects my mental model by being consistent.</p><p><strong>Contributor:</strong> Alessandro Linzi</p><p>Read more here: <a href=\"https://clig.dev\" target=\"_blank\" rel=\"noopener noreferrer\">Command Line Interface Guidelines</a></p>"
    },
    {
      "title": "Google Research 2025: Quantum Echoes and the Acceleration of Discovery",
      "slug": "google-research-2025-quantum-echoes-acceleration-discovery",
      "publishedAt": "2025-12-24T10:00:00+01:00",
      "summary": "Google Research’s 2025 retrospect highlights a 13,000x quantum simulation speedup, the rise of “Generative UI” in Gemini 3, and agentic workflows that turn AI into an active co-scientist.",
      "tags": [
        "AI",
        "Quantum Computing",
        "Research",
        "Google"
      ],
      "content": "<p>Google Research’s 2025 review frames their progress as an accelerating “magic cycle”: foundational breakthroughs fuel applied solutions, which in turn generate new data and questions for research. While corporate reviews often lean on hyperbole, this year’s technical milestones—specifically in quantum algorithms and agentic science—warrant close attention from the research community.</p><h2>Quantum Advantage via “Quantum Echoes”</h2><p>The standout announcement is the “Quantum Echoes” algorithm, running on the Willow chip. Google claims this system executes <strong>13,000 times faster</strong> than the best classical equivalent on top-tier supercomputers. The application focus here is simulating atomic interactions (specifically via nuclear magnetic resonance), a critical bottleneck in drug design and materials science. This is not just a hardware win; it’s an algorithmic leap that brings verifiable quantum advantage closer to production utility.</p><h2>The \"Science of Science\"</h2><p>We are seeing a shift from AI as a tool to AI as a collaborator. The “AI co-scientist,” a multi-agent system developed with Google DeepMind, is now generating novel hypotheses and writing expert-level empirical software to test them. </p><ul><li><strong>Stanford:</strong> The system identified potential drug repurposing candidates for liver fibrosis.</li><li><strong>Imperial College London:</strong> It derived antimicrobial resistance hypotheses in days that previously took years.</li></ul><p>This agentic approach—where AI proposes, codes, and evaluates—fundamentally changes the velocity of the scientific method.</p><h2>Generative UI and Gemini 3</h2><p>On the interaction layer, Gemini 3 introduces “Generative UI.” Rather than returning static text or pre-baked widgets, the model dynamically renders interactive interfaces (web pages, tools, visualizations) tailored to the specific query. This moves us away from rigid retrieval towards ad-hoc software generation, where the “app” exists only for the duration of the user’s intent.</p><h2>Planetary Intelligence</h2><p>The “Earth AI” initiative leverages Gemini’s reasoning to synthesize geospatial data (remote sensing, weather, mobility). A practical outcome is FireSat, a satellite constellation using AI for near-real-time wildfire detection, capable of spotting classroom-sized fires. This integrates decades of sensor modeling with modern inference to reduce the latency between observation and actionable insight.</p><p><strong>Contributor:</strong> Alessandro Linzi</p><p>Read more here: <a href=\"https://research.google/blog/google-research-2025-bolder-breakthroughs-bigger-impact/\" target=\"_blank\" rel=\"noopener noreferrer\">Google Research Blog</a></p>"
    },
    {
      "title": "AI-native assignments: using Gemini in Classroom without outsourcing thinking",
      "slug": "ai-native-assignments-gemini-classroom",
      "publishedAt": "2025-12-23T14:06:00+01:00",
      "summary": "Gemini in Classroom makes AI feel like infrastructure, not a novelty. The opportunity is real, but only if assignments are redesigned so the thinking stays visible and assessable.",
      "tags": [
        "Google",
        "AI",
        "Education"
      ],
      "content": "<p>Google is turning Gemini into a built-in layer inside Classroom, and it’s now positioned as a no-cost suite of tools for educators with Google Workspace for Education accounts, with 30+ features aimed at saving time on common teaching tasks.</p><p>That sounds like convenience, but it’s also a curriculum stress test: if the tool can generate the “school-shaped” artifact instantly, then the assignment has to move up a level and grade the process, not the polish.</p><h2>The shift: from tool-use to task design</h2><p>When AI is always available, the question isn’t “can students use it?” but “what exactly counts as learning, and how do we make it observable?” The fastest way to get this wrong is to keep the same prompts, the same rubrics, the same homework format—and just hope honesty wins.</p><p>Instead, treat AI like a new medium. The job becomes designing tasks where AI output is allowed, but shallow thinking is not.</p><h2>Three rules for AI-native assignments</h2><h3>1) Make the process gradeable</h3><p>Use a simple structure that forces students to show work:</p><ul><li><strong>Draft A (human):</strong> messy, incomplete, but honest.</li><li><strong>Draft B (AI-assisted):</strong> revised with Gemini (or any model).</li><li><strong>Decision log:</strong> 8–12 bullet points explaining what changed and why.</li></ul><p>Grade the decision log like an argument: clarity, tradeoffs, and whether the student can justify accepting or rejecting suggestions.</p><h3>2) Require verification, not just fluency</h3><p>AI makes “sounds right” dangerously cheap. So build a verification step into the deliverable:</p><ul><li>Two external sources (not AI) supporting or contradicting key claims.</li><li>A “conflict note” when sources disagree.</li><li>A final paragraph titled <em>What I’m still unsure about</em>.</li></ul><p>This turns AI output into a hypothesis generator, not an authority.</p><h3>3) Anchor authenticity with short orals</h3><p>Keep AI in homework if desired—but shift authenticity checks into class:</p><ul><li>3-minute micro-viva: explain one key choice from the decision log.</li><li>Live “debug”: fix one weak paragraph without AI.</li><li>One-minute source defense: why that source is credible.</li></ul><p>The point isn’t to punish AI use; it’s to reward understanding.</p><h2>How Classroom features map to these rules</h2><p>Gemini in Classroom is framed around helping educators kickstart lessons, differentiate materials, and generate things like quizzes and rubrics.</p><p>Used well, that time savings can be reinvested into better task constraints (the part that actually drives learning).</p><ul><li><strong>Rubrics:</strong> Generate a first draft rubric, then add explicit criteria for “decision quality” and “verification quality.”</li><li><strong>Teacher-led NotebookLM in Classroom:</strong> Create a study guide and Audio Overview grounded only in teacher-provided materials, then ask students to extract claims + evidence from that bounded set.</li><li><strong>Teacher-led Gems in Classroom:</strong> Build a “Quiz me” or “Study partner” Gem that helps students practice, but require them to cite exactly which class resource the answer came from.</li></ul><p>Google also describes upcoming or expanding analytics and standards-based tracking—tagging coursework to learning standards, viewing performance analytics, and surfacing insights like missing assignments or improving grades.</p><p>That matters because AI-native assessment often creates more small artifacts (logs, checks, orals). Analytics can help spot who is falling behind early, before the gap becomes permanent.</p><h2>A 30-minute rollout plan (realistic)</h2><p>If there’s only time for one iteration, start here:</p><ol><li>Pick one writing assignment already in the next two weeks.</li><li>Use Gemini to draft the rubric, then add 2 criteria: “Decision log quality” and “Verification quality.”</li><li>Create one bounded support artifact (NotebookLM study guide grounded in your uploaded materials).</li><li>Add a 3-minute micro-viva at the start of next lesson: each student defends one decision from their log.</li><li>Use Classroom insights/analytics to follow up with students who missed steps (not just the final submission).</li></ol><p>This approach doesn’t require perfect policy. It requires a repeatable structure.</p><h2>The hidden risk: outsourcing as a habit</h2><p>AI can make school feel easier while learning becomes thinner. The antidote is not banning, but redesigning tasks so effort shifts from typing to judgment: selecting, verifying, defending, and revising under constraints.</p><p>If students learn that “good work” means “good decisions,” AI becomes a lever for learning instead of a shortcut around it.</p><p><strong>Contributor:</strong> Alessandro Linzi</p><p>Read more here: <a href=\"https://blog.google/outreach-initiatives/education/classroom-ai-features/\" target=\"_blank\" rel=\"noopener noreferrer\">Google Blog</a></p>"
    },
    {
      "title": "Cloudflare's Graph-Based Maintenance Scheduler",
      "slug": "cloudflare-maintenance-scheduler-workers-graph",
      "publishedAt": "2025-12-23T09:00:00+01:00",
      "summary": "How Cloudflare re-architected their maintenance scheduler using a graph-based data model on Workers to handle global infrastructure constraints without OOM errors.",
      "tags": [
        "Engineering",
        "Distributed Systems",
        "Cloudflare"
      ],
      "content": "<p>I always find it fascinating to see how major infrastructure players solve their own internal tooling problems. Cloudflare recently shared how they built their automated maintenance scheduler, and the evolution of their architecture is a textbook example of hitting the limits of naive implementations.</p><p>The problem is classic constraint satisfaction: With data centers in 330+ cities, you can't just turn off a router for maintenance whenever you feel like it. You have to ensure that doing so doesn't isolate a region or break a specific customer's routing rule (like their \"Aegis\" product, which binds traffic to specific IPs).</p><h2>The \"Naive\" Approach vs. The Graph</h2><p>Their first attempt was to load all the data—server relationships, product configs, health metrics—into a single Worker. Unsurprisingly, this hit memory limits immediately. You can't fit the state of a global network into a single ephemeral function's RAM.</p><p>The fix was to shift mental models from \"loading a dataset\" to \"traversing a graph.\" They adopted an object-association model inspired by Facebook's TAO. Instead of \"give me all Aegis pools,\" the query becomes \"give me the Aegis pools associated with <em>this</em> specific datacenter.\"</p><p>This reduced response sizes by 100x, but introduced a new problem: the \"N+1 query\" issue, or what they call the \"thundering herd\" of tiny requests.</p><h2>Middleware as the Hero</h2><p>To solve the request volume, they built a fetch pipeline middleware that handles:</p><ul><li><strong>Deduplication:</strong> Merging identical in-flight requests (similar to Go's <code>singleflight</code>).</li><li><strong>LRU Caching:</strong> A tiny in-memory cache for the immediate execution context.</li><li><strong>CDN Caching:</strong> Caching GET requests at the edge with careful TTLs (1 minute for real-time data, hours for static infrastructure data).</li></ul><p>The result was a ~99% cache hit rate. It’s a great reminder that when you move to micro-requests, the network overhead becomes your new bottleneck, and caching strategy becomes your primary optimization lever.</p><h2>What I take from this</h2><p>The most interesting part for me wasn't the scheduler itself, but the pivot to <strong>Parquet files on R2</strong> for historical analysis.</p><p>Querying months of Prometheus TSDB blocks from object storage is painfully slow because of random reads. By converting that data to Parquet (a columnar format), they could issue precise range requests, fetching only the specific columns needed. They report a 15x performance improvement. It’s a strong argument for using big-data formats even in operational tooling when the scale gets large enough.</p><p><strong>Contributor:</strong> Alessandro Linzi</p><p>Read more here: <a href=\"https://blog.cloudflare.com/building-our-maintenance-scheduler-on-workers/\" target=\"_blank\" rel=\"noopener noreferrer\">Cloudflare — Building our maintenance scheduler</a></p>"
    },
    {
      "title": "Microsoft Ignite 2025: Taming the Agent Swarm",
      "slug": "microsoft-ignite-2025-agents-work-iq",
      "publishedAt": "2025-12-22T16:30:00+01:00",
      "summary": "Microsoft is pivoting from chatbots to \"Work IQ,\" trying to solve the context problem that makes most current AI annoying to use. Also: a look at the 1.3 billion agent prediction and why \"Shadow AI\" might be the next big headache.",
      "tags": [
        "AI",
        "Microsoft",
        "Agents"
      ],
      "content": "<p>We’ve all reached that frustration point with AI tools: they can write a poem about a toaster, but they have no idea who your boss is or where you saved that Q3 spreadsheet.</p><p>That missing piece is exactly what Microsoft Ignite 2025 tried to address. The keynote was heavy on metaphors—something about AI not being a \"cherry on top\" but the ice cream itself—but if you look past the marketing, they are acknowledging a hard truth: chatbots are cool, but they aren't actually <em>integrated</em> yet.</p><h2>Making AI Understand \"Work\"</h2><p>The most promising update is something they call <strong>Work IQ</strong>. It’s an attempt to give the model a memory of your actual workday—your habits, your colleagues, and the specific jargon your team uses.</p><p>This feels like the right move. The reason most enterprise AI demos fall flat is the \"cold start\" problem. You have to explain everything to the bot every time. If Work IQ actually works, it stops the AI from being a generic consultant and turns it into a team member that already has the context.</p><h2>The \"Shadow AI\" Problem</h2><p>There was one number in the announcement that actually made me sit up: they are projecting <strong>1.3 billion AI agents by 2028</strong>.</p><p>Think about that for a second. We used to worry about \"Shadow IT\"—employees using unauthorized Dropbox accounts or Trello boards. Now, imagine thousands of autonomous scripts running in the background of your company, making decisions without anyone knowing who built them or what data they can access.</p><p>Microsoft’s solution is <strong>Agent 365</strong>, which is basically an HR department for bots. It lets IT admins see, secure, and govern these agents. It sounds boring, but honestly, it’s probably the most critical feature they announced. Without it, that swarm of 1.3 billion agents is just a security nightmare waiting to happen.</p><h2>Getting the Plumbing Right</h2><p>They also introduced <strong>Fabric IQ</strong> and <strong>Foundry IQ</strong> to help connect raw data to these agents. It’s less flashy than a new LLM release, but it confirms a trend I’m seeing everywhere: the \"magic\" phase of AI is over.</p><p>We are firmly in the plumbing phase now. It’s no longer about how smart the model is; it’s about whether you can hook it up to your real data without breaking everything. Microsoft is betting that if they own the plumbing (the data layer) and the governance (the security layer), they’ll win the agent era. They're probably right.</p><p><strong>Contributor:</strong> Alessandro Linzi</p><p>Read more here: <a href=\"https://blogs.microsoft.com/blog/2025/11/18/from-idea-to-deployment-the-complete-lifecycle-of-ai-on-display-at-ignite-2025/\" target=\"_blank\" rel=\"noopener noreferrer\">Microsoft Blog</a></p>"
    },
    {
      "title": "AI Transparency Is Slipping (And That’s a Problem)",
      "slug": "ai-transparency-is-slipping",
      "publishedAt": "2025-12-22T07:27:12+01:00",
      "summary": "A new Stanford-led index finds that AI companies are sharing less information about how their flagship models are built and governed. The result is a widening transparency gap that makes independent oversight harder exactly when foundation models are becoming infrastructure.",
      "tags": [
        "AI",
        "Transparency",
        "Governance"
      ],
      "content": "<p>AI models are quickly turning into critical infrastructure: they shape how people search, write, create, and make decisions at scale. Yet a Stanford-led research team argues that the industry is moving in the opposite direction on disclosure, with companies increasingly withholding information that would enable meaningful scrutiny.</p><p>The findings come from the <em>2025 Foundation Model Transparency Index</em>, which evaluates major AI developers on a 100-point scale across areas such as training data disclosure, risk mitigation, and broader impacts. According to the index, the overall level of transparency is low, and it has declined compared to the previous year.</p><h2>What the index measures</h2><p>The Foundation Model Transparency Index is designed to score how much companies disclose about their flagship foundation models and related practices. It spans multiple dimensions—such as where data comes from, what safety processes exist, and what is known about downstream use and impacts—because transparency isn’t a single checkbox: it’s a collection of concrete, verifiable disclosures.</p><p>In 2025, the index assessed 13 companies and found large variation in disclosure practices. The average score was about 40/100, and companies tended to fall into three rough clusters: top performers around 75, a middle group around 35, and low scorers around 15.</p><h2>Big gaps between companies</h2><p>One striking result is how uneven disclosure has become. IBM sits at the top with a reported 95/100—described as the highest score in the index’s history—and is highlighted for providing unusually detailed information, including enough detail for external researchers to replicate training data practices and allowing access for external entities such as auditors.</p><p>At the other end, xAI and Midjourney are reported at 14/100, with the write-up stating they share essentially no information about training data, risks, or mitigation steps. The overall picture is not just “some companies are better than others,” but that a meaningful portion of the market is operating with minimal public accountability.</p><h2>The environmental blind spot</h2><p>The Stanford HAI write-up emphasizes a major omission across the board: environmental impact. It claims that 10 of the assessed companies disclose none of the key information related to environmental impact (including energy usage, carbon emissions, or water use), which matters because datacenter expansion and training workloads have real-world resource costs.</p><p>This is a practical transparency issue, not a philosophical one. Without standardized disclosures, it becomes difficult for policymakers, researchers, and even customers to compare footprint claims or validate sustainability commitments.</p><h2>Openness isn’t the same as transparency</h2><p>A useful distinction in the piece is that “open” (publishing model weights) doesn’t automatically mean “transparent” (explaining practices and impacts). The write-up warns that even influential open-weight developers can still be opaque about core items like training compute, risk assessment, and downstream use.</p><p>This matters because public debate often treats open-weight releases as a proxy for accountability. The index argues that assumption is risky: disclosure must be specific, structured, and repeatable to support real oversight.</p><h2>Why this matters now</h2><p>The researchers frame transparency as an “essential public good” for governance, harm mitigation, and oversight, and they point to growing policy interest in mandating disclosures for frontier AI risks. They also note that the index aims to help identify which transparency areas resist improvement without policy pressure.</p><p>Even if one disagrees with any single score, the direction of travel is the key signal: as foundation models become more central to economies and institutions, baseline disclosure is not keeping up.</p><p><strong>Contributor:</strong> Alessandro Linzi</p><p>Read more here: <a href=\"https://hai.stanford.edu/news/transparency-in-ai-is-on-the-decline\" target=\"_blank\" rel=\"noopener noreferrer\">Stanford Edu News</a></p>"
    },
    {
      "title": "Continuous Efficiency: AI as a daily habit for greener software",
      "slug": "continuous-efficiency-ai-greener-software",
      "publishedAt": "2025-12-21T19:05:00+01:00",
      "summary": "Sustainability rarely wins sprint planning, but efficiency always matters. Continuous Efficiency is an attempt to make energy-aware optimization incremental, automated, and part of everyday development.",
      "tags": [
        "AI",
        "Sustainability",
        "Green Software",
        "GitHub"
      ],
      "content": "<p>When was the last time someone asked in a standup: <em>“How could we do this more sustainably?”</em> I don’t hear it often—not because developers don’t care, but because time is scarce and the backlog is loud.</p><p>The GitHub Next and GitHub Sustainability teams propose a framing that feels more actionable: treat sustainability as <strong>efficiency</strong>. Faster code, less waste, fewer resources burned. Their name for the practice is <em>Continuous Efficiency</em>: incremental, validated improvements that accumulate over time instead of arriving as a big “green refactor” that never gets scheduled.</p><h2>Why “continuous” is the point</h2><p>Most teams already have habits that run continuously: tests, linters, formatting, security checks, CI. Continuous Efficiency tries to join that family. The promise is not magic optimization; it’s that small improvements can happen regularly, be measured, and be reviewed—so the codebase gently trends toward being cheaper to run and easier to maintain.</p><h2>Where AI fits</h2><p>The key enabling idea is combining Continuous AI (automation enriched by modern LLMs inside collaboration workflows) with Green Software practices (building systems that are more energy-efficient and typically more performant and resilient). In the best case, this turns sustainability from “extra work” into “default behavior”.</p><h2>Agentic workflows as a delivery mechanism</h2><p>One concrete implementation discussed is an experimental approach called Agentic Workflows running in GitHub Actions. The interesting part is authoring: instead of writing traditional automation logic, you describe intent in natural language inside a Markdown file, then compile it into a standard workflow. At runtime an agent can inspect repository context, propose changes, and surface them through familiar collaboration artifacts like comments and pull requests—while staying within the guardrails of the platform.</p><h2>Standards, rules, and performance work</h2><p>Two directions stand out. First, turning standards into executable rules: describe what “good” looks like, then apply it across a codebase more broadly than conventional pattern-based tooling. Second, performance improvement in heterogeneous real-world repos: an iterative approach where an agent can discover how to build and benchmark a project, run measurements, and propose targeted optimizations under human guidance.</p><p>I like this because it doesn’t pretend expertise disappears. Instead, it tries to scale it: making improvements easier to start, easier to validate, and easier to repeat—so teams can keep learning in public, but also keep their software lean.</p>\n\n<p><strong>Contributor:</strong> Alessandro Linzi</p>\n\n<p>Read more here: <a href=\"https://github.blog/news-insights/policy-news-and-insights/the-future-of-ai-powered-software-optimization-and-how-it-can-help-your-team/\" target=\"_blank\" rel=\"noopener noreferrer\">GitHub Blog</a></p>"
    },
    {
      "title": "Bloom is a factory for behavioral evaluations",
      "slug": "bloom-factory-for-behavioral-evaluations",
      "publishedAt": "2025-12-21T13:28:00+01:00",
      "summary": "Anthropic’s Bloom is an agentic pipeline that turns a vague safety concern into a measurable evaluation suite in days. The key idea is simple: generate many scenarios that try to elicit a behavior, run them at scale, then score the transcripts—so you can quantify how often (and how severely) a model slips into patterns you care about.",
      "tags": [
        "AI",
        "Safety",
        "Evaluation",
        "Anthropic"
      ],
      "content": "<p>One of the frustrating truths about AI safety is that “we should evaluate that” is often the start of a months-long detour. You need prompts, scenarios, transcripts, scoring rubrics, infrastructure, and then you discover your evaluation is either too easy, too gameable, or already obsolete.</p>\n\n<p>Anthropic’s <em>Bloom</em> is a direct response to that pain: a pipeline meant to generate behavioral evaluations quickly, for arbitrary behaviors, and to output numbers you can track over time.</p>\n\n<h2>What Bloom is trying to solve</h2>\n<p>Behavioral evaluations matter most when they measure something messy and real: deception, sabotage, self-preservation, sycophancy, bias, “evaluation awareness”, and other traits that do not show up cleanly in typical benchmark QA.</p>\n\n<p>The problem is that building these evaluations by hand is slow, and once they exist they can go stale. Models train on similar data, capabilities shift, and what once was a strong test turns into a predictable obstacle course.</p>\n\n<p>Bloom’s bet is that evaluation creation itself should be automated and <em>regenerated</em> repeatedly, so you measure the same underlying behavior but with fresh scenarios each time.</p>\n\n<h2>The core idea: an assembly line for evaluations</h2>\n<p>Bloom is structured like an agentic assembly line that starts with a researcher’s description of a target behavior and ends with a scored evaluation suite.</p>\n\n<p>At a high level, the pipeline has four stages:</p>\n<ul>\n<li><strong>Understanding</strong>: interpret what the behavior means and what “counts” as evidence for it.</li>\n<li><strong>Ideation</strong>: generate many scenarios designed to elicit the behavior.</li>\n<li><strong>Rollout</strong>: run those scenarios, simulating users (and sometimes tools) to produce transcripts.</li>\n<li><strong>Judgment</strong>: score transcripts for presence/severity of the behavior and summarize suite-level metrics.</li>\n</ul>\n\n<p>This structure matters because it separates “what we want to measure” from “how we elicit it” and from “how we judge it”. That modularity is what makes iteration fast.</p>\n\n<h2>Why I find this useful</h2>\n<p>Bloom reads like an attempt to make alignment measurement feel more like engineering than philosophy. Instead of arguing abstractly about whether a model is safe, you can track an elicitation rate, compare runs, and monitor regressions after model updates.</p>\n\n<p>Another subtle benefit: because scenarios can be generated anew each run (while still being reproducible via a seed), Bloom is less dependent on a fixed evaluation set. That helps reduce the “teaching to the test” dynamic where benchmarks slowly become training targets.</p>\n\n<h2>The practical workflow shift</h2>\n<p>What changes for practitioners is not just speed, but cadence. Bloom encourages a loop like:</p>\n<ul>\n<li>Pick a behavior you actually worry about in deployment.</li>\n<li>Generate a first suite, inspect failures, refine the behavior description and configuration.</li>\n<li>Run at scale, compare across models, and keep the seed as the thing you cite and rerun.</li>\n</ul>\n\n<p>That’s much closer to how teams maintain reliability in production systems: frequent tests, updated test cases, clear metrics.</p>\n\n<p><strong>Contributor:</strong> Alessandro Linzi</p>\n\n<p>Read more here: <a href=\"https://www.anthropic.com/research/bloom\" target=\"_blank\" rel=\"noopener noreferrer\">Anthropic Research: Bloom</a>.</p>"
    },
    {
      "title": "Google Research 2025 Is a Strategy Document Disguised as a Recap",
      "slug": "google-research-2025-recap-strategy-document",
      "publishedAt": "2025-12-20T13:55:00+01:00",
      "summary": "Google’s 2025 Research recap reads less like a year-end blog post and more like a map of where they think the frontier is moving. The throughline is a tightening loop: foundational work ships into products, products create new constraints and data, and that pressure drives the next research wave. What stood out wasn’t one breakthrough, but the system-level pattern: efficiency, factuality, multimodality, interactive interfaces, and domain tools are being built as one stack.",
      "tags": [
        "AI",
        "Research",
        "Google",
        "Infrastructure"
      ],
      "content": "<p>A year-in-review from a major lab is rarely just a recap. It’s closer to a strategy document: a curated list of what they want you to believe matters, and a hint of what they plan to compound next.</p><p>Google Research frames 2025 as an accelerating “magic cycle” where research turns into products faster, and products generate new needs that shape the next research agenda. Read that literally and it’s corporate storytelling. Read it operationally and it’s a useful model of how modern AI progress actually compounds: deploy, measure, adapt, repeat.</p><h2>The stack is tightening</h2><p>The most revealing part of the recap is that it treats generative AI as a full-stack system, not a single model upgrade. The headline improvements are about making models more efficient, more factual, more multilingual and multi-cultural, and more capable across modalities (images, audio, video, 3D). That list matters because it’s a description of where real-world friction lives: cost, correctness, global usability, and robustness outside text-only sandboxes.</p><p>There’s also a strong retrieval theme: Google highlights work on retrieval-augmented generation and even the idea that a system can detect when it has “enough information” to answer correctly. That’s an important mindset shift, because it reframes retrieval as a control problem (when to stop, when to abstain, when to ask for more), not just “let’s add a search box to the model.”</p><h2>Factuality is becoming an engineering discipline</h2><p>Plenty of labs talk about truthfulness. Google’s recap reads like they’re trying to turn factuality into something closer to an engineering discipline: benchmarks, datasets, uncertainty signaling, and mechanisms for grounding in external context.</p><p>The deeper point is that “being factual” isn’t a single property of a model. It’s an end-to-end behavior that emerges from training, retrieval, evaluation, and how the product decides to present an answer. If this is right, then the next competitive advantage won’t come from who can generate the nicest paragraph—it’ll come from who can build systems that reliably know what they know.</p><h2>Generative UI is the quiet platform shift</h2><p>The recap’s most product-shaped idea is “generative UI”: models generating interactive interfaces (web pages, games, tools, apps) in response to a prompt. That sounds like a gimmick until you realize what it implies: the model isn’t just outputting text, it’s outputting a usable artifact that changes what the user can do next.</p><p>This matters because interfaces are leverage. If a model can produce a small interactive tool that constrains the problem, collects the right inputs, and surfaces the right outputs, the user stops “prompting” and starts operating a mini-application. That’s a different workflow, and it’s one reason AI products are drifting toward interactive, multimodal experiences instead of chat boxes.</p><h2>Science is getting agentified</h2><p>The scientific side of the recap is ambitious: multi-agent systems like an “AI co-scientist” for hypothesis generation, plus coding-agent tooling meant to help scientists write and iterate empirical software. The framing is consistent: reduce the cycle time of research by turning the overhead (searching, synthesizing, coding, re-running) into something that can be parallelized and automated.</p><p>Even if you discount the big claims, the direction is clear: AI is being positioned less as a universal oracle and more as a workflow accelerator that can run many small, checkable steps fast. If this pattern holds, the scientists who benefit most won’t be the ones who ask better questions—they’ll be the ones who instrument their work so the model can actually help.</p><h2>Planetary intelligence is a product category now</h2><p>Another thread that feels distinctly “Google” is Earth-scale intelligence: geospatial reasoning, crisis resilience, wildfire and flood forecasting, and climate tools that ship into real surfaces. What’s interesting is not that Google has models—it’s that they’re blending remote sensing, weather, maps, and reasoning into products meant to generate actionable insights.</p><p>This is also where the “magic cycle” becomes visible: deployment forces evaluation. When a system is used for flood alerts, fire detection, or cyclone forecasts, the cost of being wrong is explicit, and the feedback loop becomes sharper than any benchmark.</p><p><strong>Contributor:</strong> Alessandro Linzi</p>\n\n<p>Read more here: <a href=\"https://research.google/blog/google-research-2025-bolder-breakthroughs-bigger-impact/\" target=\"_blank\" rel=\"noopener noreferrer\">Google Research 2025</a></p>"
    },
    {
      "title": "Monitoring AI Reasoning: Can We Trust What Models Think?",
      "slug": "monitoring-ai-reasoning-chain-of-thought-monitorability",
      "publishedAt": "2025-12-19T17:30:00+01:00",
      "summary": "OpenAI released a framework to evaluate whether AI reasoning traces are legible, faithful, and useful for oversight. The work introduces 13 evaluations across three archetypes—intervention, process, and outcome-property—to measure if chain-of-thought actually reveals what models are doing.",
      "tags": [
        "AI",
        "Safety",
        "OpenAI"
      ],
      "content": "<p>Reasoning models that think out loud via chain-of-thought (CoT) create a natural opening for oversight: if we can see intermediate steps, we can spot problems before they become actions. But that only works if the thinking trace is honest, interpretable, and predictive of behavior. OpenAI's new monitorability framework directly tackles that question: are CoT traces actually useful for monitoring, or just plausible-sounding narratives?</p>\n\n<h2>Three ways to test monitorability</h2>\n<p>The evaluation suite organizes 13 distinct tests across 24 environments into three archetypes, each probing a different aspect of how well reasoning traces support human oversight:</p>\n<ul>\n<li><strong>Intervention tests:</strong> Can we steer or interrupt problematic reasoning patterns? If the trace reflects genuine deliberation, targeted interventions should shift behavior predictably.</li>\n<li><strong>Process tests:</strong> Do thinking traces represent actual reasoning steps, or are they post-hoc justifications? This checks for faithfulness—whether the model's internal process matches what it writes.</li>\n<li><strong>Outcome-property tests:</strong> Do more monitorable thoughts correlate with safer or more accurate outcomes? If reasoning quality predicts result quality, monitoring becomes actionable.</li>\n</ul>\n\n<h2>Why this matters for deployment</h2>\n<p>Advanced reasoning models are increasingly used in agentic settings where they execute multi-step workflows with real consequences. Monitoring becomes critical in domains where reward-hacking, unsafe shortcuts, or subtle misalignment could be catastrophic.</p>\n\n<p>OpenAI's companion paper shows this isn't hypothetical: they successfully monitored o3-mini for reward hacking in agentic coding environments using GPT-4o as the monitor—meaning a weaker model could effectively oversee a stronger one by observing its chain-of-thought.</p>\n\n<h2>The fragility problem</h2>\n<p>One key finding is that monitorability isn't automatic or stable. As models scale up test-time compute or undergo reinforcement learning, reasoning traces can become longer without becoming more faithful. The evaluation framework helps developers track whether increased thinking effort actually improves oversight or just adds noise.</p>\n\n<p>This is particularly important because a convincing-sounding trace isn't proof of reliable reasoning—it could be optimized to look good to evaluators while hiding the actual decision process.</p>\n\n<h2>What changes for practitioners</h2>\n<p>The practical takeaway is to measure monitorability as a first-class property, not assume it comes for free with CoT. Teams building with reasoning models should:</p>\n<ul>\n<li>Run monitorability evaluations periodically, especially after scaling compute or applying RL tuning.</li>\n<li>Prefer process-based rewards in safety-critical tasks to incentivize correct reasoning steps, not just final answers.</li>\n<li>Document governance policies: when CoT is stored, who accesses it, retention periods, and escalation protocols.</li>\n</ul>\n\n<h2>The bigger shift</h2>\n<p>This work signals a broader evolution in how we think about AI safety. Instead of treating models as black boxes evaluated only on outputs, the focus shifts to making internal processes observable and steerable. If reasoning traces are legible and faithful, they become a control surface—a way to intervene before bad outcomes materialize.</p>\n\n<p>That's a powerful idea, but only if the traces actually reflect what the model is doing. OpenAI's framework gives us a systematic way to check.</p>\n\n<p><strong>Contributor:</strong> Alessandro Linzi</p>\n\n<p>Read more here: <a href=\"https://openai.com/index/evaluating-chain-of-thought-monitorability/\" target=\"_blank\" rel=\"noopener noreferrer\">Open AI Blog</a>.</p>"
    },
    {
      "title": "Gemini in secondary school: the tool is easy — the transformation isn’t",
      "slug": "gemini-secondary-school-tool-easy-transformation-isnt",
      "publishedAt": "2025-12-19T12:10:00+01:00",
      "summary": "Gemini and other AI tools are becoming “normal” inside Google for Education, especially through Classroom. The hard part is no longer access — it’s designing learning that still makes students think, and building a culture where AI is used with judgment instead of convenience.",
      "tags": [
        "AI",
        "Education",
        "Google for Education"
      ],
      "content": "<p>There’s a point where a new tool stops feeling like a novelty and starts feeling like infrastructure. That’s where Google for Education seems to be heading with Gemini: not “an extra app,” but a layer that sits inside the workflows teachers and students already use.</p>\n\n<p>And that’s exactly why it’s worth thinking about it in secondary school. When AI is everywhere, the interesting question isn’t “can it help?” but <em>what kind of thinking does it quietly replace</em> — and what kind of thinking it can amplify if we’re intentional.</p>\n\n<h2>The promise: leverage, not magic</h2>\n<p>On paper, the use cases are obvious. Teachers are overloaded, students need feedback loops, and secondary school is full of bottlenecks where motivation dies: the blank page, the first draft, the fear of being wrong.</p>\n\n<ul>\n<li>For teachers, Gemini-style tools can draft lesson plans, generate quiz questions, and create differentiated materials faster than a human can do from scratch.</li>\n<li>For students, the same tools can act like a tutor that never gets tired: re-explaining a concept, giving examples, generating practice questions, or helping structure an argument.</li>\n</ul>\n\n<p>But the real promise isn’t that AI will “teach.” It’s that it can reduce friction enough that teachers spend more time on the parts of teaching that are <em>irreducibly human</em>: noticing misunderstanding, building trust, designing meaningful tasks, and helping students form an identity as learners.</p>\n\n<h2>The first deep challenge: motivation vs. outsourcing</h2>\n<p>Secondary school is the stage where students learn what “work” means. They also learn shortcuts. AI makes the best shortcut in history: instant coherence, instant structure, instant confidence.</p>\n\n<p>That creates a new kind of risk: not cheating as a moral failure, but <em>outsourcing as a habit</em>. If students repeatedly skip the painful early phase of thinking — the messy, uncertain, half-formed draft — they can end up with polished text and shallow understanding.</p>\n\n<p>This is the uncomfortable part: the more helpful the tool becomes, the more the curriculum has to shift from “produce an artifact” to “show the process.” Otherwise, assessment quietly becomes a contest of who can delegate best.</p>\n\n<h2>The second deep challenge: epistemic trust</h2>\n<p>In a classroom, authority is usually visible: textbooks, teachers, sources, citations. With AI, authority becomes conversational. It sounds confident, it speaks fluently, it rarely says “I don’t know.”</p>\n\n<p>So a student doesn’t just learn content — they learn a new relationship with knowledge itself. If an answer can be generated instantly, what becomes valuable is not recall, but the ability to judge: to cross-check, to detect weak reasoning, to separate “plausible” from “true.”</p>\n\n<p>AI literacy in secondary school can’t just be about prompt tips. It has to include a culture of verification: students learning to treat AI output as a <em>draft hypothesis</em>, not a fact.</p>\n\n<h2>What changes for teachers</h2>\n<p>Teacher workload is a real, practical reason to care about this. If AI can shrink the time spent on routine prep and repetitive feedback, that’s not a gimmick — it’s a structural improvement.</p>\n\n<p>But teachers also become designers of constraints. The job shifts from “explain and assign” toward “design tasks where AI use is visible, bounded, and educational.”</p>\n\n<ul>\n<li>Ask for students’ decision logs: why they accepted or rejected suggestions.</li>\n<li>Use oral checks and micro-vivas for authenticity.</li>\n<li>Grade the quality of sources, assumptions, and argument structure — not just the final polish.</li>\n</ul>\n\n<p>In other words: the teacher becomes less of a content broadcaster, more of a thinking coach.</p>\n\n <p><strong>Contributor:</strong> Alessandro Linzi</p>\n\n<p>Read more here: <a href=\"https://blog.google/outreach-initiatives/education/classroom-ai-features/\" target=\"_blank\" rel=\"noopener noreferrer\">Gemini in Classroom: No-cost AI tools that amplify teaching</a>.</p>"
    },
    {
      "title": "Guardrails for when AI gets personal",
      "slug": "guardrails-for-when-ai-gets-personal",
      "publishedAt": "2025-12-18T21:37:00+01:00",
      "summary": "AI safety is easy to discuss and hard to operationalize. Anthropic’s latest update is interesting because it focuses on shipped safeguards and how they measure whether those safeguards work, especially in emotionally loaded conversations. The post centers on two areas: handling suicide and self-harm topics with care, and reducing sycophancy (the model telling users what they want to hear). The bigger point is that “helpful” isn’t only about better answers, it’s also about preventing predictable failure modes.",
      "tags": [
        "AI",
        "Safety",
        "Evaluation",
        "Anthropic"
      ],
      "content": "<p>AI safety gets abstract quickly, so it’s refreshing when a lab talks about the boring part: what they actually built, how they tested it, and where it still falls short.</p><p>Anthropic’s update is focused on user well-being in conversations where the stakes are real. The theme running through it is practical: combine training, product interventions, and evaluations that match messy real-world usage.</p><h2>When the topic is self-harm</h2><p>The core idea is simple: a chatbot shouldn’t act like a therapist, but it also shouldn’t respond coldly or carelessly when someone is struggling. The post describes a mix of model behavior shaping and product-level safeguards designed to route people toward human support when needed.</p><p>What matters here is not just having a policy, but having mechanisms that trigger reliably in ambiguous situations, where intent can be unclear and the conversation can drift over time.</p><h2>Measuring the hard cases</h2><p>One point worth highlighting is how they evaluate: single-turn prompts, multi-turn scenarios, and stress tests that start mid-conversation. That last category is especially important because many failures happen after the model has already “committed” to a tone or framing and has to course-correct without escalating the situation.</p><p>This is the right direction for safety evaluation: less about cherry-picked prompts, more about dynamics across time and uncertainty.</p><h2>Sycophancy is a safety issue</h2><p>The other half of the update focuses on sycophancy: the tendency to be overly agreeable, flattering, or to mirror the user even when it’s not true or helpful. In normal contexts it’s annoying; in reality-disconnected contexts it can actively reinforce bad outcomes.</p><p>The interesting tension is that warmth and friendliness can be a feature, but if it comes at the expense of truth-seeking and gentle pushback, it turns into a reliability problem.</p><p><strong>Contributor:</strong> Alessandro Linzi</p>\n\n<p>Read more here: <a href=\"https://www.anthropic.com/news/protecting-well-being-of-users\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.anthropic.com/news/protecting-well-being-of-users</a></p>"
    },
    {
      "title": "Verified AI is the opposite of vibes",
      "slug": "verified-ai-is-the-opposite-of-vibes",
      "publishedAt": "2025-12-18T15:00:00+01:00",
      "summary": "Most AI systems are persuasive even when they are wrong. That is fine for brainstorming, but it breaks down fast in science and engineering. Axiomatic AI is building a different stack: AI grounded in logic, proofs, and physics models. The idea is not to generate a plausible answer, but to return results that can be checked.",
      "tags": [
        "AI",
        "Mathematics",
        "Verification",
        "Research"
      ],
      "content": "<p>Most AI feels like magic until it matters. The moment a result feeds into a design decision, a publication, or a piece of production infrastructure, “sounds plausible” stops being a feature and starts being a liability.</p><p>Axiomatic AI’s pitch is refreshingly blunt: build AI grounded in logic, evidence, and the scientific method, where outputs are mathematically verified and traceable, not just probable. In their framing, the goal is <em>no hallucinations</em>—because the system doesn’t return an answer unless it can be verified.</p><h2>From probability to proof</h2><p>The key shift is to treat verification as part of the computation, not as something humans do afterward. Axiomatic AI highlights using formal proof tools (notably Lean 4) to check logical soundness and mathematical rigor. That means mathematics isn’t “explained” in natural language; it’s expressed in a formal system that a computer can check.</p><p>This matters because a lot of scientific and engineering failures are not dramatic conceptual mistakes—they’re subtle errors that propagate: a wrong assumption, a missing constraint, a sign mistake, an approximation silently applied outside its regime. The whole point of a theorem prover is to make those failures loud.</p><h2>Physics as a guardrail</h2><p>Another part of the stack is grounding in physics-based models (their examples mention fundamental equations like Maxwell’s and Schrödinger’s) so predictions respect physical laws and can be validated against known solutions. The idea is simple: if a model is going to claim something about the physical world, it should be constrained by the structure of the physical world.</p><p>It’s less “generate an answer” and more “compute within a framework where breaking the laws of physics is not allowed.”</p><h2>Specialized agents, formal interfaces</h2><p>Axiomatic AI also describes specialized agents for different domains (math, physics, engineering) that communicate through formal interfaces—e.g., a math agent verifies equations before a physics agent runs a simulation. That’s an underrated systems idea: modular reasoning is only useful if the modules can’t lie to each other. Formal interfaces are how you get that.</p><p>In other words, “agentic” isn’t the headline here; <em>verifiable coordination</em> is.</p><h2>Measurement as a first-class output</h2><p>What really completes the picture is that they’re not only talking about proofs. They also talk about experimental control: integrating AI with hardware workflows (via CloudLab and their AX platform), using adaptive experimentation and Bayesian optimization to explore parameter spaces efficiently, and logging measurement conditions for reproducibility.</p><p>This is where “trustworthy AI” stops being abstract. If the system can suggest an experiment, run it, collect data, and record every knob it touched, the result becomes something you can reproduce, not just something you can screenshot.</p><h2>AX Verified Research and provenance</h2><p>Finally, AX Verified Research™ is basically an opinionated answer to the reproducibility crisis: track every result through knowledge graphs that capture data lineage, experimental conditions, and verification status. Their examples mention Neo4j to represent relationships between datasets, analyses, and publications, and to query which artifacts contributed to a given result.</p><p>This is the part that feels most “infrastructure”: not just correctness, but traceability. If a plot is wrong, it should be possible to walk backward to the dataset, the transformation, the version, and the assumptions—without archaeology.</p><p><strong>Contributor:</strong> Alessandro Linzi</p>\n\n<p>Read more here: <a href=\"https://axiomatic-ai.com/technology/axiomatic-intelligence/\" target=\"_blank\" rel=\"noopener noreferrer\">Axiomatic AI Website</a></p>"
    },
    {
      "title": "Why Proof Assistants Are Suddenly Practical",
      "slug": "why-proof-assistants-are-suddenly-practical",
      "publishedAt": "2025-12-17T15:27:00+01:00",
      "summary": "Proof assistants used to feel like tools for specialists, mostly useful when you had the time and patience to formalize everything. That’s changing. A recent post by Terence Tao shows a more lightweight direction: interactive, tactic-driven proving inside Python, designed to certify the kinds of estimates and inequalities that show up constantly in real math work.",
      "tags": [
        "Mathematics",
        "Proof Assistants",
        "Formal Verification"
      ],
      "content": "<p>Proof assistants have a reputation for being powerful but slow: the kind of thing you reach for when you want absolute certainty, and you’re willing to pay for it in time and friction. Terence Tao’s recent experiment is interesting because it attacks that tradeoff directly, aiming for a workflow where “formal-ish” verification is fast enough to be used as part of everyday problem solving, not just as a final archival step.</p><h2>A proof assistant that feels like a REPL</h2><p>Tao describes iterating from a proof-of-concept verifier into a more flexible, extensible proof assistant that deliberately mimics Lean in key ways, while being implemented in Python and powered by SymPy. The interaction model is intentionally simple: run Python in interactive mode, load an exercise, and drive the proof forward by applying tactics to the current proof state.</p><p>The examples make the point quickly. A goal is presented as a structured state (variables, hypotheses, and a target), and a single tactic like <code>Linarith()</code> can close a linear arithmetic goal, with an optional verbose mode that exposes the underlying feasibility check. It’s the same idea as modern theorem provers: humans provide high-level moves, the system does the tedious bookkeeping.</p><h2>Why this matters: the “boring” middle of proofs</h2><p>What makes this direction compelling is not that it replaces full formalization. It’s that it tries to cover the annoying middle ground: proofs that are conceptually straightforward but algebraically messy, where most human error hides. Tao explicitly leans toward semi-automated interactive proofs, where the user provides tactics and the tool pushes calculations through until the goal is discharged.</p><p>This is also why the tool’s focus on estimates is a good litmus test. Real analysis and asymptotics are full of manipulations that are easy to get wrong in small ways, and expensive to formalize end-to-end in a fully verified foundation. A lightweight assistant that can reliably certify these steps could be a practical bridge between pen-and-paper reasoning and fully formal proof libraries.</p><h2>Asymptotics, SymPy, and a pragmatic foundation</h2><p>The post gets especially fun when it moves from propositional/linear reasoning into asymptotic estimates. Tao sketches an approach where orders of magnitude (like <code>Theta(X)</code>) are represented in a way that plays nicely with SymPy’s symbolic machinery, and then verified via a log-linear arithmetic solver. In other words: take a domain where humans often hand-wave, and build tactics that turn the hand-waving into checkable structure.</p><p>There’s an important honesty here too. Tao notes the tool is not designed to be fully formally sound—Python and SymPy are not certified kernels—so the promise is not “trusted down to axioms.” The promise is closer to: get something that works, build a corpus of tactic-driven proofs, and eventually translate or export certificates into a fully verified system (he mentions Lean as a target direction) once the workflow is stable enough to justify the cost.</p><h2>The bigger picture</h2><p>One way to read this is as a bet on proof tooling evolving like developer tooling. A strict, verified kernel is like a compiler backend; a flexible, interactive layer is like the frontend that makes humans productive. If proof assistants are going to matter outside of niche communities, they need to feel less like writing a second proof and more like debugging: iterative, inspectable, and fast enough that you actually use it.</p><p><strong>Contributor:</strong> Alessandro Linzi</p>\n\n<p>Read more here: <a href=\"https://terrytao.wordpress.com/2025/05/09/a-tool-to-verify-estimates-ii-a-flexible-proof-assistant/\" target=\"_blank\" rel=\"noopener noreferrer\">Terence Tao — “A tool to verify estimates, II: a flexible proof assistant”</a>.</p>"
    },
    {
      "title": "DeepMind and the UK AI Security Institute: making safety more measurable",
      "slug": "deepmind-uk-ai-security-institute-making-safety-more-measurable",
      "publishedAt": "2025-12-16T16:00:00+01:00",
      "summary": "DeepMind is expanding its partnership with the UK AI Security Institute (AISI) under a new MoU, shifting from one-off model testing toward deeper joint safety and security research. The focus areas include monitoring model “thinking” (chain-of-thought), studying social and emotional harms from misalignment, and exploring economic impacts via task simulations.",
      "tags": [
        "AI",
        "Safety",
        "Policy",
        "DeepMind",
        "UK"
      ],
      "content": "<p>AI safety can sound like philosophy until it turns into something operational: shared access, shared methods, and shared measurements. That’s what stood out in Google DeepMind’s announcement about deepening its partnership with the UK AI Security Institute (AISI) through a new Memorandum of Understanding focused on foundational security and safety research.</p><p>The key shift is moving beyond “test the model” moments toward longer-running research collaboration — the kind of work that can accumulate into better evaluation tooling over time.</p><h2>What the partnership includes</h2><p>DeepMind frames the updated partnership around a few practical commitments:</p><ul><li>Sharing access to proprietary models, data, and ideas to accelerate research progress.</li><li>Joint reports and publications to share findings with the broader research community.</li><li>More collaborative security and safety research and ongoing technical discussions.</li></ul><p>This is the boring-but-important infrastructure layer of safety: not just finding issues, but building the machinery to keep finding them.</p><h2>Three areas they’ll focus on</h2><p>The announcement calls out three research directions that feel especially relevant as models become more capable:</p><ul><li><strong>Monitoring AI reasoning processes</strong>: techniques for tracking a system’s “thinking,” often described as chain-of-thought monitoring, to better understand how answers are produced.</li><li><strong>Social and emotional impacts</strong>: work on “socioaffective misalignment,” where a system can follow instructions but still behave in ways that don’t align with human wellbeing.</li><li><strong>Economic systems</strong>: simulating real-world tasks, having experts score them, and using that to reason about longer-term labour market impacts.</li></ul><p>None of this is a silver bullet, but it’s a sign that frontier AI safety is increasingly treated like an engineering and measurement problem — something you can improve with better tools, not just better intentions.</p><p><strong>Contributor:</strong> Alessandro Linzi</p>\n\n<p>Read more here: <a href=\"https://deepmind.google/blog/deepening-our-partnership-with-the-uk-ai-security-institute/\" target=\"_blank\" rel=\"noopener noreferrer\">Google DeepMind</a>.</p>"
    },
    {
      "title": "When AI learns how to learn: DiscoRL and the automation of RL algorithms",
      "slug": "when-ai-learns-how-to-learn-discorl-automating-rl-algorithms",
      "publishedAt": "2025-12-15T16:48:00+01:00",
      "summary": "Reinforcement learning has powered many of the biggest AI milestones, but the learning rules behind it are still mostly hand-designed. A new Nature paper shows a different path: let machines discover the learning rule itself, by meta-learning from the experience of many agents across many environments. The result is a discovered rule (DiscoRL) that beats existing learning rules on Atari and transfers surprisingly well to other benchmarks it never saw during discovery. The interesting shift is not a single benchmark win, but a change in how progress could happen.",
      "tags": [
        "AI",
        "Reinforcement Learning",
        "Research",
        "DeepMind"
      ],
      "content": "<p>Reinforcement learning (RL) sits under a lot of the iconic AI wins of the last decade. But there’s an irony in how it’s usually built: the agent learns through experience, while the learning rule itself is still mostly handcrafted by humans. A new Nature paper argues that this split is becoming unnecessary — and demonstrates a system that can <em>discover</em> a high-performing RL update rule from experience at scale.</p><p>The headline result is bold but easy to appreciate: the discovered rule (called <strong>DiscoRL</strong>) outperformed existing learning rules on the classic Atari benchmark, and then held up well on other challenging benchmarks that weren’t part of the discovery process. The deeper point is the direction of travel: if learning rules can be learned, algorithm design starts to look less like artisanal engineering and more like something that can compound with compute and diverse experience.</p><h2>The idea in plain terms</h2><p>Most RL algorithms differ in how they update an agent’s policy and predictions after it takes actions and receives rewards. In this work, instead of choosing that update rule upfront, the researchers represent the rule as a trainable “meta-network” that outputs targets the agent should move toward — effectively learning <em>how</em> the agent should update itself.</p><p>They then run a population of agents across many environments, and continuously improve the meta-network so that the agents trained under it achieve higher returns. Over time, this process produces a learning rule that is competitive with, and in some cases better than, the manually designed rules the field has relied on.</p><h2>Why this matters</h2><p>If this approach scales, it changes the bottleneck. Instead of relying on slow human iteration to invent new update rules, you can search a much larger space of possible algorithms using experience and meta-optimization — and let the resulting rule generalize beyond the environments it was discovered on.</p><p>The paper also makes a practical point: the discovered rule improves as discovery uses more diverse and complex environments, which hints at an “algorithm scaling law” style dynamic — better rules as a function of experience diversity, not just model size. That’s a big deal for anyone thinking about general-purpose agents.</p><h2>What stood out (without the math)</h2><p>A few pieces are worth calling out even without diving into technical details. First, the authors report that DiscoRL surpassed prior approaches on Atari in their setup, and that a variant discovered on a larger and more diverse environment set improved performance on multiple other benchmarks.</p><p>Second, their analysis suggests the learned rule develops its own useful internal predictions that don’t map cleanly onto standard RL concepts like “value functions,” and that these learned predictions end up informing the policy update rather than staying as a side quest. In other words: it’s not just rediscovering the same tricks with different knobs.</p><h2>The bigger takeaway</h2><p>This is a glimpse of a future where AI systems don’t just learn tasks — they also learn the learning machinery that makes them effective. That doesn’t mean research becomes automatic, but it does suggest progress may shift from “invent a new rule” to “design a discovery process that reliably produces strong rules.”</p><p><strong>Contributor:</strong> Alessandro Linzi</p>\n\n<p>Read more here: <a href=\"https://www.nature.com/articles/s41586-025-09761-x\" target=\"_blank\" rel=\"noopener noreferrer\">Nature</a>.</p>"
    },
    {
      "title": "Perplexity at Work: a simple model for getting more done",
      "slug": "perplexity-at-work-simple-model-get-more-done",
      "publishedAt": "2025-12-15T09:00:00+01:00",
      "summary": "Perplexity at Work argues AI productivity fails less from weak models and more from fragmented workflows. It proposes a three-step approach: reclaim focus by reducing context switching, scale your output with integrated research/creation tools, then convert that leverage into measurable results through repeatable automations like shortcuts and scheduled tasks.",
      "tags": [
        "AI",
        "Productivity",
        "Work",
        "Perplexity"
      ],
      "content": "<p>AI productivity doesn’t fail because the models are weak. It fails because modern work is already fragmented: too many tabs, too many apps, too many interruptions, too many tiny handoffs that drain attention. <em>Perplexity at Work</em> is interesting because it treats AI as a workflow design problem, not a “prompting” problem.</p><p>The guide frames productive work as a progression in three layers: first you reclaim focus, then you scale your capabilities, and finally you convert that leverage into measurable results. The point is not to add another tool to manage, but to remove the friction that keeps you reacting all day instead of building anything substantial.</p><h2>Block distractions</h2><p>The foundational move is getting your attention back. The guide argues that the biggest productivity win comes from eliminating the admin overhead and context switching that constantly pulls you out of deep work. That’s where Perplexity’s workflow concept shows up: instead of bouncing between email, docs, calendar, research tabs, and internal tools, you delegate the repetitive glue tasks to AI.</p><p>Two practical ideas stood out:</p><ul><li>Use an AI assistant as an “attention shield”: summarize, triage, and surface what actually needs action.</li><li>Collapse multi-step workflows into a single prompt so you don’t pay the mental tax of switching tools and re-orienting.</li></ul><h2>Scale yourself</h2><p>Once focus returns, AI becomes a force multiplier. The guide’s core claim is that AI is best when your own talent stays in the lead: you bring the goals, taste, judgment, and constraints; AI brings speed, synthesis, and execution support. Instead of treating research and creation as separate phases, you can keep context connected and iterate faster.</p><p>Perplexity’s toolkit is presented as a unified platform (rather than scattered subscriptions), with components like an AI browser for research and actions, a research agent that reads broadly and cites sources, a creation studio for deliverables, and spaces to keep context organized across projects. The consistent theme: keep everything in one working environment so the context follows you.</p><h2>Get results</h2><p>The final layer is where most “AI productivity” talk gets vague, but this guide keeps it grounded: results are about outcomes other people recognize. That could be shipping faster, creating clearer deliverables, building better proposals, or showing impact in performance reviews. The idea is to channel the extra bandwidth into visible wins rather than just doing more busywork.</p><p>The guide encourages turning recurring work into automation primitives:</p><ul><li>Shortcuts for repeatable multi-step routines you trigger on demand.</li><li>Scheduled tasks for recurring research and reporting so the updates happen without you remembering to ask.</li></ul><p>That’s how AI stops being a clever assistant and starts functioning like a quiet operations layer.</p><h2>A better prompt habit</h2><p>A subtle but important point: prompting works best when you “think out loud” from the goal, not the keywords. Strong prompts describe the outcome, the workflow steps, and the format—so the assistant can execute like a capable teammate, not a search box.</p><p>In practice, that means asking for sequences (“first do X, then do Y, then produce Z”), and reusing those sequences as templates for the work you do every week.</p><p><strong>Contributor:</strong> Alessandro Linzi</p>\n\n<p>Read more here: <a href=\"https://www.perplexity.ai/enterprise/perplexity-at-work\" target=\"_blank\" rel=\"noopener noreferrer\">Perplexity at Work</a>.</p>"
    },
    {
      "title": "MCP for Google services: the missing piece for real AI automation",
      "slug": "mcp-google-services-missing-piece-ai-automation",
      "publishedAt": "2025-12-14T14:00:00+01:00",
      "summary": "Google is rolling out fully-managed, remote MCP servers so AI agents can reliably use Google Cloud and Google services as tools. The shift is subtle but big: models stop being just “smart text” and become systems that can plan and act across real infrastructure with governance.",
      "tags": [
        "AI",
        "Agents",
        "Automation",
        "Google Cloud"
      ],
      "content": "<p>The big blocker for “agentic AI” hasn’t been intelligence. It’s been <em>reliable tool use</em>: how a model can safely read data, call APIs, and take actions without fragile glue code. Google’s announcement of official Model Context Protocol (MCP) support for Google services is a practical step toward that future, because it turns huge parts of the Google ecosystem into standardized, discoverable tools for agents.</p>\n\n<h2>MCP as the connector layer</h2>\n<p>MCP (Model Context Protocol) is described as a kind of “USB‑C for AI”: a standard way for models to connect to tools and data. The promise is less about smarter responses and more about completing multi-step tasks in the real world, where answers depend on current data, permissions, and operational constraints.</p>\n\n<p>The pain point Google calls out is that community MCP servers often require developers to install and manage local servers, or deploy open-source solutions themselves, which can be fragile and burdensome. Google’s move is to provide fully-managed, remote MCP servers so developers can point their agents (or standard MCP clients) at a consistent endpoint across Google and Google Cloud services.</p>\n\n<h2>What “official, managed MCP servers” changes</h2>\n<p>This is an automation upgrade disguised as plumbing. Instead of every team wiring their own set of connectors, Google is adding MCP as a unified layer on top of existing API infrastructure.</p>\n\n<p>In practice, it means an agent can do the boring-but-critical parts of work more reliably:</p>\n<ul>\n<li>Discover which tools exist (and what they do) through a standard interface.</li>\n<li>Use tools with structured inputs/outputs, instead of scraping text from CLIs.</li>\n<li>Operate with enterprise governance instead of “just trust the prompt.”</li>\n</ul>\n\n<h2>First services in scope</h2>\n<p>Google says MCP support is rolling out incrementally, starting with several high-impact services:</p>\n<ul>\n<li><strong>Google Maps</strong>, via Maps Grounding Lite, to ground agents in trusted geospatial data (places, weather forecasts, routing, distance, travel time) and reduce hallucinations on location queries.</li>\n<li><strong>BigQuery</strong>, to let agents interpret schemas and run queries directly against enterprise data while keeping data in-place and governed (including access to features like forecasting).</li>\n<li><strong>Compute Engine</strong>, so agents can provision/resize infrastructure and handle day‑2 operations like adapting to changing workloads.</li>\n<li><strong>GKE</strong>, so agents can interact with Kubernetes APIs through a structured interface (less brittle parsing), enabling diagnosis, remediation, and cost optimization with guardrails.</li>\n</ul>\n\n<h2>Security and observability: where automation becomes usable</h2>\n<p>Automation only becomes deployable when it’s governable. Google highlights a “find trusted tools + control access” approach: Cloud API Registry and Apigee API Hub for discovery, Google Cloud IAM for access control, audit logging for observability, and Model Armor to help defend against agentic threats like indirect prompt injection.</p>\n\n<p>That’s important because it reframes what an “agent” is in an enterprise setting: not a clever model, but a controlled operator that leaves logs, follows permissions, and uses approved tools.</p>\n\n<h2>The brain shift: from asking to delegating</h2>\n<p>There’s a subtle cognitive shift that happens as tools become more reliable. When software is brittle, people keep tasks in their head and use tools as assistants. When tool use becomes robust and standardized, people start thinking in goals and delegations.</p>\n\n<p>MCP pushes in that direction: instead of “write me a query,” the task becomes “find the best retail location,” and the agent coordinates BigQuery analysis with Maps validation as one workflow. Google even sketches that exact example: an agent built with Agent Development Kit, backed by Gemini 3 Pro, forecasting revenue in BigQuery while cross-referencing Maps to scout nearby businesses and validate routes—all via managed MCP servers.</p>\n\n<p>This is the kind of change that compounds: not because any single model call is magical, but because the cost of connecting intelligence to action keeps dropping.</p>\n\n<p><strong>Contributor:</strong> Alessandro Linzi</p>\n\n<p>Read more here: <a href=\"https://cloud.google.com/blog/products/ai-machine-learning/announcing-official-mcp-support-for-google-services\" target=\"_blank\" rel=\"noopener noreferrer\">Google Cloud announces official MCP support for Google services.</a>.</p>"
    },
    {
      "title": "AI agents for smart cities: from monitoring to action",
      "slug": "ai-agents-smart-cities-from-monitoring-to-action",
      "publishedAt": "2025-12-14T10:00:00+01:00",
      "summary": "NVIDIA's AI agents go beyond monitoring city cameras—they actively respond to incidents, reroute traffic, and coordinate emergency responses in real time.",
      "tags": [
        "AI",
        "Agents",
        "Infrastructure"
      ],
      "content": "<p>NVIDIA's latest work on smart city AI agents moves beyond passive monitoring. These aren't just detection systems scanning camera feeds; they're active decision-makers that respond to urban incidents in real time.</p>\n\n<h2>From detection to coordinated response</h2>\n<p>The core idea is simple but powerful: connect city cameras to AI agents that don't just flag problems, but act on them. When an agent detects a traffic accident, it doesn't stop at alerting dispatch—it coordinates the full response:</p>\n<ul>\n<li>Identifies the incident location and severity from video feeds.</li>\n<li>Notifies first responders with precise coordinates and context.</li>\n<li>Reroutes traffic signals to clear paths for ambulances.</li>\n<li>Updates digital signage and navigation apps for drivers.</li>\n</ul>\n\n<p>This orchestration turns scattered city systems into a unified response network.</p>\n\n<h2>The agent architecture</h2>\n<p>Each agent specializes in a domain but collaborates through a central coordinator:</p>\n<ul>\n<li><strong>Perception agents:</strong> Analyze camera feeds for accidents, crowds, infrastructure failures.</li>\n<li><strong>Decision agents:</strong> Prioritize responses based on urgency and available resources.</li>\n<li><strong>Action agents:</strong> Interface with traffic lights, dispatch systems, public alerts.</li>\n<li><strong>Learning agents:</strong> Refine detection accuracy and response protocols over time.</li>\n</ul>\n\n<p>Running on NVIDIA hardware, the system processes multiple video streams simultaneously while maintaining low latency for time-critical decisions.</p>\n\n<h2>Real-world deployment patterns</h2>\n<p>Cities aren't starting from scratch. The agents integrate with existing infrastructure:</p>\n<ul>\n<li>Traffic management systems (signals, VMS boards).</li>\n<li>Public safety networks (police, fire dispatch).</li>\n<li>Navigation APIs (Waze, Google Maps).</li>\n<li>Emergency medical services coordination.</li>\n</ul>\n\n<p>The value compounds: faster response times reduce accident severity, cleared traffic paths save lives, and learned patterns improve future predictions.</p>\n\n<h2>What scales beyond traffic</h2>\n<p>The same agent architecture applies to other urban challenges:</p>\n<ul>\n<li><strong>Crowd management:</strong> Detect unsafe densities at events, suggest dispersal routes.</li>\n<li><strong>Infrastructure monitoring:</strong> Spot road damage, bridge stress, utility failures.</li>\n<li><strong>Public safety:</strong> Flag suspicious activity, coordinate multi-agency responses.</li>\n<li><strong>Environmental response:</strong> Monitor flooding, air quality, deploy mitigation.</li>\n</ul>\n\n<p>Once deployed, agents learn city-specific patterns, making the system smarter without constant human retuning.</p>\n\n<p><strong>Contributor:</strong> Alessandro Linzi</p>\n\n<p>Read more here: <a href=\"https://blogs.nvidia.com/blog/smart-city-ai-agents-urban-operations/\" target=\"_blank\" rel=\"noopener noreferrer\">NVIDIA AI agents for smart city operations</a>.</p>"
    },
    {
      "title": "Codex: the self-improving AI coding agent",
      "slug": "codex-self-improving-ai-coding-agent",
      "publishedAt": "2025-12-14T09:00:00+01:00",
      "summary": "OpenAI's Codex isn't just a coding assistant—it's a cloud-based agent that writes, debugs, and improves itself. Four engineers built the Sora Android app in under a month using it.",
      "tags": [
        "AI",
        "Coding",
        "OpenAI"
      ],
      "content": "<p>OpenAI's Codex is a cloud-based AI coding agent that handles everything from writing features to debugging code. Launched as a research preview in May 2025, it's available through ChatGPT, VS Code, and a CLI that's drawing comparisons to Anthropic's Claude Code.</p>\n\n<h2>The self-improvement loop</h2>\n<p>What sets Codex apart is its recursive development: OpenAI engineers use it to enhance Codex itself. This isn't theoretical—the loop is delivering real results. Four engineers built the entire Sora Android app in under a month using Codex for most of the heavy lifting.</p>\n\n<p>The agent operates across interfaces but shines in the CLI, where developers report 10x productivity gains on routine tasks. Usage spiked after the CLI release, showing external developers are adopting it fast.</p>\n\n<h2>Not replacement, amplification</h2>\n<p>Codex works as a \"junior developer\" that handles boilerplate, debugging, and implementation details. Humans focus on architecture, complex logic, and creative problem-solving. The pattern is clear:</p>\n<ul>\n<li><strong>Routine tasks:</strong> Codex writes CRUD endpoints, fixes syntax errors, implements standard patterns.</li>\n<li><strong>Human oversight:</strong> Review architecture decisions, edge cases, security implications.</li>\n<li><strong>Iteration:</strong> Codex learns from feedback and code reviews to improve future outputs.</li>\n</ul>\n\n<p>This isn't automation replacing engineers; it's leverage that lets small teams ship at scale.</p>\n\n<h2>Real-world validation</h2>\n<p>The Sora Android app is the proof point: a production app built rapidly by a tiny team. Codex handled the bulk of implementation while humans shaped the product direction. External developers report similar gains—faster prototyping, fewer bugs in early iterations, more time for high-level design.</p>\n\n<p>The self-improvement aspect means Codex gets better over time, not just from model updates but from learning the specific patterns and preferences of individual teams.</p>\n\n<h2>What changes for developers</h2>\n<p>The shift is from \"writing code\" to \"orchestrating code generation.\" Engineers become more like conductors: defining requirements clearly, reviewing outputs critically, and iterating rapidly. The bottleneck moves from implementation speed to problem framing and validation.</p>\n\n<p>CLI adoption suggests this pattern scales beyond OpenAI. When any developer can spin up a self-improving coding agent, the barrier to building complex software drops significantly.</p>\n\n<p><strong>Contributor:</strong> Alessandro Linzi</p>\n\n<p>Read more here: <a href=\"https://arstechnica.com/ai/2025/12/how-openai-is-using-gpt-5-codex-to-improve-the-ai-tool-itself/?utm_source=perplexity\" target=\"_blank\" rel=\"noopener noreferrer\">How OpenAI is using GPT-5 + Codex to improve the AI tool itself</a>.</p>"
    },
    {
      "title": "WeatherNext 2: Trying to Make the Atmosphere Less Chaotic (for Us)",
      "slug": "weathernext-2-trying-to-make-the-atmosphere-less-chaotic",
      "publishedAt": "2025-12-09T09:15:00+01:00",
      "summary": "Google DeepMind’s WeatherNext 2 pushes global AI forecasting toward hourly resolution, runs up to 8× faster, and can generate hundreds of coherent scenarios—useful when the worst-case path matters more than the average.",
      "tags": [
        "AI",
        "Climate",
        "Research"
      ],
      "content": "\n    <p>\n      Weather is the original adversarial dataset: messy, nonlinear, and extremely good at punishing overconfidence. Most days the question isn’t “will it rain?” but “how wrong can the forecast be, and how costly is that error?” WeatherNext 2 is interesting because it doesn’t just chase a single best-guess forecast—it tries to map the space of plausible futures fast enough to be useful.\n    </p>\n\n    <h2>What’s new in WeatherNext 2</h2>\n\n    <p>\n      Google DeepMind and Google Research describe WeatherNext 2 as their most advanced global forecasting system, with the headline improvement being speed: it can generate forecasts up to 8× faster, and at up to 1-hour time resolution. Faster matters because it changes the practical bottleneck: instead of spending compute on one forecast, you can spend it on many scenarios.\n    </p>\n\n    <h2>From one future to many</h2>\n\n    <p>\n      The most provocative idea here is that a single deterministic forecast is often the wrong product. WeatherNext 2 can generate hundreds of possible weather outcomes from a single starting point, and it does it in under a minute per scenario on a single TPU (as described in the announcement). That’s the kind of capability that turns forecasting into decision support: “what’s the distribution of outcomes?” rather than “what’s the one answer?”\n    </p>\n\n    <p>\n      This is enabled by a modeling approach they call a Functional Generative Network (FGN), which injects “noise” into the model in a way intended to keep forecasts physically realistic and internally consistent. In plain terms: it’s not randomizing pixels; it’s sampling coherent worlds that still obey the constraints of weather systems.\n    </p>\n\n    <h2>Where it shows up (and why that matters)</h2>\n\n    <p>\n      WeatherNext technology is already being integrated into consumer-facing surfaces: Google says it has upgraded weather forecasts in Search, Gemini, Pixel Weather, and Google Maps Platform’s Weather API, with Google Maps integration planned as well. That’s a big deal because the value of better forecasts isn’t only scientific—it’s logistical, operational, and very human (commutes, travel plans, outdoor work, safety decisions).\n    </p>\n\n    <h2>Opening it up to researchers</h2>\n\n    <p>\n      Beyond products, WeatherNext 2 forecast data is being made available via Earth Engine and BigQuery, and Google also mentions an early access program on Vertex AI for custom inference. If this becomes accessible to more researchers and developers, it could accelerate downstream work: impact modeling, risk tools, and domain-specific forecasting layers built on top of a strong global prior.\n    </p>\n\n    <p>\n      The optimistic take is not “we’ve solved weather” (we haven’t), but that forecasting can become more trustworthy by being more explicit about uncertainty. In a chaotic system, honesty about the range of plausible outcomes is often the closest thing to reliability.\n    </p>\n\n    <p><strong>Contributor:</strong> Alessandro Linzi</p>\n\n<p>\n      Read more here: <a href=\"https://blog.google/technology/google-deepmind/weathernext-2/\">Google DeepMind Blog</a>\n    </p>\n  "
    },
    {
      "title": "AI and Alzheimer’s: not a cure, but a smarter way to fight",
      "slug": "ai-and-alzheimers-not-a-cure-but-a-smarter-way-to-fight",
      "publishedAt": "2025-12-11T09:00:00+01:00",
      "summary": "AI isn’t curing Alzheimer’s yet, but it’s already changing how we find and test treatments. Two recent examples show how AI can match the right patients to the right drugs and uncover hidden cases in routine health records.",
      "tags": [
        "AI",
        "Life Sciences",
        "Alzheimer's"
      ],
      "content": "<p>AI isn’t curing Alzheimer’s disease yet, but it’s already reshaping how we fight it. Instead of waiting for a magic bullet, the real progress is in using AI to make existing drugs more effective, trials more efficient, and diagnosis more equitable. Two recent examples capture this shift well.</p>\n\n<h2>AI that matches the right patients to the right drugs</h2>\n<p>A team at the University of Cambridge used an AI model to re-analyse a completed Alzheimer’s clinical trial that had failed in the overall population. The AI could predict, from early cognitive and imaging data, which patients were slow vs. rapid progressors toward full-blown Alzheimer’s.</p>\n\n<p>When they re-ran the trial data through this lens, they found something striking: the drug slowed cognitive decline by 46% in a subgroup of early-stage, slow-progressing patients with mild cognitive impairment. In the other group, it didn’t help.</p>\n\n<p>The takeaway isn’t that this drug is a cure; it’s that AI can identify which patients are most likely to benefit. That means smaller, cheaper, faster trials, and a move toward precision medicine: matching the right drug to the right patient at the right time.</p>\n\n<h2>AI that finds undiagnosed cases in routine records</h2>\n<p>At UCLA, researchers built an AI tool that scans electronic health records to flag patients with undiagnosed Alzheimer’s. This addresses a huge gap: Alzheimer’s is significantly underdiagnosed, especially in underrepresented communities.</p>\n\n<p>Their model uses a semi-supervised approach that’s designed to be fair across different populations. It looks at patterns in diagnoses, age, and clinical notes, and can pick up subtle signals (like certain comorbidities) that might otherwise be missed.</p>\n\n<p>When validated, it showed much higher sensitivity across racial/ethnic groups than traditional models, and patients flagged as high-risk had higher genetic risk scores for Alzheimer’s. The goal isn’t to replace clinicians, but to help them prioritize who needs a deeper evaluation, especially as new disease-modifying treatments become available.</p>\n\n<h2>What this means for the Alzheimer’s fight</h2>\n<p>These examples show that near-term value of AI in Alzheimer’s isn’t about autonomous discovery, but about making the human-driven process smarter and more equitable.</p>\n\n<p>On the drug side, AI helps us:</p>\n<ul>\n<li>Rescue promising drugs that failed in broad trials by finding responsive subgroups.</li>\n<li>Design smaller, more efficient trials that target the right patients.</li>\n<li>Move toward a precision medicine approach where treatment is tailored to individual progression risk.</li>\n</ul>\n\n<p>On the care side, AI helps us:</p>\n<ul>\n<li>Reduce diagnostic disparities by flagging high-risk patients in routine records.</li>\n<li>Enable earlier intervention, when lifestyle changes and new therapies can have the most impact.</li>\n<li>Scale detection in primary care without adding huge burdens on clinicians.</li>\n</ul>\n\n<p>AI won’t replace neurologists or drug developers, but it can make them much more effective. The real win is not a single breakthrough, but a system that’s faster, fairer, and more precise.</p>\n\n<p><strong>Contributor:</strong> Alessandro Linzi</p>\n\n<p>Read more here: <a href=\"https://www.cam.ac.uk/research/news/ai-can-accelerate-search-for-more-effective-alzheimers-medicines-by-streamlining-clinical-trials\" target=\"_blank\" rel=\"noopener\">AI can accelerate search for more effective Alzheimer’s medicines by streamlining clinical trials</a> and <a href=\"https://www.uclahealth.org/news/release/researchers-develop-ai-tool-identify-undiagnosed-alzheimers\" target=\"_blank\" rel=\"noopener\">Researchers develop AI tool to identify undiagnosed Alzheimer’s</a>.</p>"
    },
    {
      "title": "El Salvador’s Nationwide AI Tutoring Program: What’s Been Announced",
      "slug": "el-salvador-nationwide-ai-tutoring-program-whats-been-announced",
      "publishedAt": "2025-12-12T10:30:00+01:00",
      "summary": "El Salvador and xAI announced a two-year plan to roll out Grok-based tutoring across 5,000+ public schools, aiming to reach over one million students and support teachers with curriculum-aligned, adaptive help.",
      "tags": [
        "AI",
        "Education",
        "Policy"
      ],
      "content": "\n    <p>\n      A notable education announcement landed this week: El Salvador and xAI say they’re launching what they describe as the first nationwide AI-powered education program. The plan is to deploy Grok across more than 5,000 public schools over the next two years, with the stated goal of delivering personalized tutoring to over one million students and support for teachers.\n    </p>\n\n    <h2>What the program aims to do</h2>\n\n    <p>\n      The core idea is an “adaptive tutor” that aligns with the national curriculum and adjusts to each student’s pace and current level. If implemented well, that kind of personalization could matter most in the long tail of learning: students who move faster than the class average, students who need extra repetition, and students in settings where teacher-to-student ratios make 1:1 help hard.\n    </p>\n\n    <p>\n      The announcement also emphasizes that the tool is meant to work alongside educators, not in isolation—positioning it as something that can support teachers with explanations, practice, and targeted reinforcement rather than replace classroom instruction.\n    </p>\n\n    <h2>Why this rollout is unusual</h2>\n\n    <p>\n      Plenty of schools experiment with AI tutors, but doing it at national scale changes the problem. It turns “does this help in a pilot?” into questions like: how do you ensure curriculum fit, consistency across regions, equitable access, and safe defaults for minors? A rollout to 5,000+ schools forces those operational and governance issues to become first-class engineering requirements.\n    </p>\n\n    <h2>What to watch next</h2>\n\n    <p>\n      The big unknowns are in the details: evaluation methods, guardrails, teacher training, how student data is handled, and how the system behaves under real classroom constraints (limited connectivity, device availability, different grade levels, and language needs). If El Salvador publishes frameworks or measurement outcomes from this deployment, those could become a reference point for other governments exploring similar programs.\n    </p>\n\n    <p><strong>Contributor:</strong> Alessandro Linzi</p>\n\n<p>\n      Read more here: <a href=\"https://x.ai/news/el-salvador-partnership\">xAI News</a>\n    </p>\n  "
    },
    {
      "title": "Perplexity Memory: Personalization Without Repeating Yourself",
      "slug": "perplexity-memory-personalization-without-repeating-yourself",
      "publishedAt": "2025-12-05T14:45:00+01:00",
      "summary": "Perplexity’s new memory layer makes AI assistants feel continuous across sessions: it can recall preferences, preload relevant context, and keep personalization even when switching models—while staying user-controlled and optional.",
      "tags": [
        "AI",
        "Productivity",
        "Perplexity"
      ],
      "content": "\n    <p>\n      One friction point in everyday AI use is surprisingly basic: repeating yourself. You explain your preferences, your ongoing project, your constraints—then a week later you’re back at zero because the context window is gone. Perplexity’s “AI assistants with memory” is a direct attempt to fix that by making context persistent across conversations.\n    </p>\n\n    <h2>What “memory” changes in practice</h2>\n\n    <p>\n      The interesting part isn’t just that the assistant can remember details—it’s that the system can preload relevant context so you don’t keep paying the “re-explain tax” every time you open a new thread. In theory that means preferences like dietary needs, favorite brands, or recurring topics become part of your default working setup, not something you restate manually.\n    </p>\n\n    <p>\n      Perplexity positions this differently from “training on your chats”: rather than treating your history as generic training data, it retrieves specific, relevant items from your memory store to answer the question you’re asking now. That’s a subtle but important distinction, because it makes memory feel like a user-controlled context layer rather than an opaque model update.\n    </p>\n\n    <h2>Privacy and control</h2>\n\n    <p>\n      A memory feature only works if it’s controllable. Perplexity emphasizes that memory can be turned off, and that memory and search history are automatically disabled in incognito mode (and prompts in incognito aren’t retained for memory). Data is encrypted, and there’s also an option to opt out of contributing to model improvement via data retention settings.\n    </p>\n\n    <h2>Context portability across models</h2>\n\n    <p>\n      Another underrated benefit is “context portability”: being able to switch between different models without losing the personalization you’ve built up. That matters because model choice is increasingly task-dependent—sometimes a fast model is enough, sometimes a reasoning model is better, and sometimes a specialized model wins—yet the context you’ve accumulated shouldn’t reset each time you change engines.\n    </p>\n\n    <p>\n      If this works well, it pushes assistants closer to something people actually want: not a single brilliant conversation, but a long-running relationship with your projects, preferences, and working style—without forcing you to trade away privacy to get it.\n    </p>\n\n    <p><strong>Contributor:</strong> Alessandro Linzi</p>\n\n<p>\n      Read more here: <a href=\"https://www.perplexity.ai/hub/blog/introducing-ai-assistants-with-memory\">Perplexity Blog</a>\n    </p>\n  "
    },
    {
      "title": "How People Actually Use AI Agents (It’s Mostly Cognitive Work)",
      "slug": "how-people-actually-use-ai-agents-mostly-cognitive-work",
      "publishedAt": "2025-12-11T16:20:00+01:00",
      "summary": "A December 2025 study from Perplexity and Harvard researchers suggests AI agents are used more as thinking partners than digital concierges, with 57% of activity focused on productivity/workflow and learning/research.",
      "tags": [
        "AI",
        "Agents",
        "Research"
      ],
      "content": "\n    <p>\n      There’s a popular story about AI agents as “digital concierges” that book hotels, schedule meetings, and run errands. Useful, sure—but also a bit narrow. A recent study from Perplexity and Harvard researchers (released in December 2025) looks at real usage at scale and lands on a different picture: agents are increasingly used as cognitive partners, not just task-runners.\n    </p>\n\n    <h2>Cognitive work dominates</h2>\n\n    <p>\n      The headline finding is striking: 57% of agent activity is cognitive work, split between Productivity &amp; Workflow (36%) and Learning &amp; Research (21%). In other words, a lot of people aren’t delegating “boring chores” as much as they’re delegating the messy middle of knowledge work: synthesizing information, navigating complexity, and turning scattered inputs into decisions.\n    </p>\n\n    <p>\n      That maps well to real examples: a professional scanning case studies to extract patterns, or a student using an agent to navigate course material and make it more searchable and explainable. This is less about avoiding work and more about compressing the overhead that normally slows work down.\n    </p>\n\n    <h2>How usage evolves over time</h2>\n\n    <p>\n      Another useful insight is the progression. New users tend to start with low-stakes queries (travel ideas, trivia, entertainment), then shift toward higher-leverage uses once they see what’s possible—debugging code, summarizing reports, planning complex workflows, or structuring learning. The study describes this as a “pull” toward productivity: once people experience the leverage, they don’t fully go back.\n    </p>\n\n    <h2>Who sticks with agents</h2>\n\n    <p>\n      Adoption isn’t uniform across professions. Digital technologists lead in volume (30% of queries), but knowledge-intensive fields like Marketing, Sales, Management, and Entrepreneurship show high “stickiness”—usage intensity that outpaces their adoption share once they integrate agents into daily workflow.\n    </p>\n\n    <p>\n      Context matters too: personal use accounts for 55% of queries, followed by professional (30%) and educational (16%). That mix is a reminder that “agent value” isn’t only about enterprise automation; it’s also about making everyday life and learning less cognitively expensive.\n    </p>\n\n    <h2>Why this matters</h2>\n\n    <p>\n      The most interesting implication is that the near-term impact of agents might be about scaling cognition rather than replacing labor. If agents primarily accelerate synthesis, learning, and workflow setup, then the economic shift looks like “hybrid intelligence”: people + tools that extend attention, memory, and speed.\n    </p>\n\n    <p><strong>Contributor:</strong> Alessandro Linzi</p>\n\n<p>\n      Read more here: <a href=\"https://www.perplexity.ai/hub/blog/how-people-use-ai-agents\">Perplexity Blog</a>\n    </p>\n  "
    },
    {
      "title": "AI Fluency: A Better Way to Work with AI (Beyond Prompt Hacks)",
      "slug": "ai-fluency-better-way-to-work-with-ai-beyond-prompt-hacks",
      "publishedAt": "2025-12-07T11:00:00+01:00",
      "summary": "Anthropic’s free AI Fluency course shifts the focus from prompt tricks to a durable collaboration framework built around the “4Ds”: Delegation, Description, Discernment, and Diligence.",
      "tags": [
        "AI",
        "Learning",
        "Productivity"
      ],
      "content": "\n    <p>\n      There’s a whole mini-industry around “the perfect prompt,” but most of those tricks decay fast: models change, interfaces change, and the hack stops working. What’s more useful is a stable mental model for collaborating with AI across tools and contexts.\n    </p>\n\n    <p>\n      Anthropic’s free AI Fluency course takes that route. Instead of teaching a bag of prompt hacks, it teaches a framework for working with AI effectively, efficiently, ethically, and safely, built around four core competencies (the “4Ds”). The course is developed in partnership with academic experts Joseph Feller and Rick Dakan.\n    </p>\n\n    <h2>The 4Ds that make it practical</h2>\n\n    <p>\n      The framework is simple enough to remember, but deep enough to apply repeatedly:\n    </p>\n\n    <ul>\n      <li>\n        <strong>Delegation</strong>: Decide what should be done by you, what should be done by the model, and how to split tasks so you don’t outsource judgment by accident.\n      </li>\n      <li>\n        <strong>Description</strong>: Communicate intent and constraints clearly (context, audience, format, examples), so the model has something concrete to aim for.\n      </li>\n      <li>\n        <strong>Discernment</strong>: Evaluate outputs critically—both the final result and how the model got there—so you can catch errors, weak logic, and hidden assumptions.\n      </li>\n      <li>\n        <strong>Diligence</strong>: Use AI responsibly: be transparent where needed, stay accountable for what you publish, and consider downstream impacts.\n      </li>\n    </ul>\n\n    <h2>Why this approach scales</h2>\n\n    <p>\n      What I like about this is that it’s tool-agnostic. The same habits apply whether the UI says Claude, ChatGPT, Grok, or something else—because the bottleneck isn’t the brand, it’s how well you set up the collaboration and how seriously you verify what comes back.\n    </p>\n\n    <p>\n      The course also leans into practice: interactive exercises, real-world projects, and a “Bad Prompt Makeover” style activity that forces you to notice why vague requests create vague results. There’s also a completion certificate, which is a nice forcing function if finishing courses usually drifts to the bottom of the todo list.\n    </p>\n\n    <h2>What this is really about</h2>\n\n    <p>\n      AI is already reshaping workflows and job roles, but “better prompting” isn’t the endgame. The people who get leverage will be the ones who can delegate strategically, communicate precisely, evaluate ruthlessly, and stay responsible for outcomes.\n    </p><p><strong>Contributor:</strong> Alessandro Linzi</p>\n\n    <p>\n      You can enroll for free in the course here: <a href=\"https://anthropic.skilljar.com/ai-fluency-framework-foundations\">Anthropic Academy</a>\n    </p>"
    },
    {
      "title": "The Illusion of Thinking: What Reasoning Models Get Right (and Where They Break)",
      "slug": "illusion-of-thinking-reasoning-models-problem-complexity",
      "publishedAt": "2025-12-10T15:10:00+01:00",
      "summary": "A NeurIPS 2025 paper argues that “reasoning” models don’t fail gracefully: performance can collapse past a complexity threshold, and extra token budget doesn’t automatically buy better thinking.",
      "tags": [
        "AI",
        "LLMs",
        "Research"
      ],
      "content": "\n    <p>\n      Reasoning models look like a big step forward: they generate a long chain of intermediate steps, then land on an answer. On math and coding benchmarks, that often works. But a question keeps bothering me: are these models actually getting better at reasoning, or are they just performing well on the kinds of problems we already know how to measure?\n    </p>\n\n    <p>\n      A NeurIPS 2025 paper called <em>The Illusion of Thinking</em> tackles that question using controllable puzzle environments. The key trick is that the puzzles let researchers dial up compositional complexity while keeping the underlying logic consistent, so you can study not only final accuracy, but also what happens inside the “thinking trace” as problems get harder.\n    </p>\n\n    <h2>Why problem complexity matters</h2>\n\n    <p>\n      Most evaluations emphasize “did the model get the right answer?” on well-known benchmark distributions. The paper argues that this is incomplete (and sometimes misleading), because it doesn’t reveal how reasoning behavior changes when you systematically push difficulty beyond the familiar range.\n    </p>\n\n    <p>\n      By controlling complexity directly, the authors can observe when a model’s apparent reasoning ability is robust and when it starts to behave more like pattern-matching under stress.\n    </p>\n\n    <h2>Key findings that stood out</h2>\n\n    <ol>\n      <li>\n        Accuracy can collapse once problems cross a certain complexity threshold, rather than degrading gradually.\n      </li>\n      <li>\n        The scaling behavior is counter-intuitive: “reasoning effort” (often measured via how much the model writes/uses its thinking tokens) increases with complexity up to a point, then drops as tasks get even harder—even when token budget is still available.\n      </li>\n      <li>\n        Under matched inference compute, the paper describes three regimes:\n        <ul>\n          <li>\n            Low complexity: standard LLMs can outperform LRMs, suggesting extra “thinking” can add noise when tasks are easy.\n          </li>\n          <li>\n            Medium complexity: LRMs tend to do better, where structured intermediate reasoning actually helps.\n          </li>\n          <li>\n            High complexity: both approaches can fail badly, highlighting a fundamental limitation rather than a tuning issue.\n          </li>\n        </ul>\n      </li>\n      <li>\n        Exact computation remains a weak spot: the traces often look heuristic and inconsistent, which is a red flag for tasks that require algorithmic, deterministic steps.\n      </li>\n    </ol>\n\n    <h2>What this changes for evaluation</h2>\n\n    <p>\n      The takeaway isn’t “reasoning models are useless.” It’s that we should be more careful about what we infer from benchmark wins. If accuracy collapses beyond a complexity boundary, then “more tokens” or “more compute at inference” isn’t a universal fix—and it becomes important to test models in settings where difficulty is controlled, not just sampled.\n    </p>\n\n    <p>\n      It also reinforces something that’s easy to forget: a convincing chain-of-thought can be a UI artifact, not proof of stable internal computation. If the trace quality degrades or becomes inconsistent as complexity rises, the model may be narrating a path rather than executing one.\n    </p>\n\n    <h2>Why I’m paying attention</h2>\n\n    <p>\n      This line of work feels important because it forces a sharper definition of “reasoning.” If a model can only reason inside a comfort zone, then the real problem becomes: how do we build systems that fail predictably, expose uncertainty, and reliably handle tasks that demand exactness?\n    </p>\n\n    <p><strong>Contributor:</strong> Alessandro Linzi</p>\n\n<p>\n      Read more here: <a href=\"https://machinelearning.apple.com/research/illusion-of-thinking\">Apple ML Research</a>\n    </p>\n  "
    },
    {
      "title": "Exploring Quantum Chaos: The Power of OTOCs",
      "slug": "exploring-quantum-chaos-otocs",
      "publishedAt": "2025-11-29T13:40:00+01:00",
      "summary": "A look at Out-of-Time-Order Correlators (OTOCs), why they’re a useful lens on quantum chaos, and how Google’s Quantum Echoes experiments connect verifiable outputs with beyond-classical computation.",
      "tags": [
        "Quantum Computing",
        "Google Research",
        "Physics"
      ],
      "content": "\n    <h2>OTOCs, a Practical Lens on Quantum Chaos</h2>\n    <p>\n      Quantum chaos is a strange topic: you’re not tracking a single trajectory like in classical mechanics, but a web of probability amplitudes evolving together.\n      OTOCs (Out-of-Time-Order Correlators) are one of the cleanest tools to probe how “scrambling” happens in these systems, meaning how local information gets spread across many degrees of freedom.\n    </p>\n    <p>\n      What makes OTOCs especially interesting in a quantum-computing context is that they’re expectation values — the kind of output that can be cross-checked across different devices and, in some cases, against physics itself — instead of a one-off bitstring from a single run.\n      That “verifiability” is a big deal when the goal is to claim results that aren’t just hard, but also checkable.\n    </p>\n\n    <h2>Quantum Echoes in Plain Terms</h2>\n    <p>\n      The core idea behind Google Quantum AI’s Quantum Echoes algorithm is to evolve a system forward in time (a unitary evolution), introduce controlled perturbations, and then evolve it back, using this forward/back structure to access an OTOC expectation value.\n      Conceptually, it feels like asking: if the system is pushed slightly during the evolution, how much does that “echo” survive when trying to rewind the dynamics?\n    </p>\n    <p>\n      Framed this way, OTOCs become a kind of quantitative “butterfly effect” for quantum systems — not because the outcome is a classical trajectory that diverges, but because the interference structure of the quantum state becomes increasingly complex as scrambling grows.\n    </p>\n\n    <h2>Why This Beats Classical Simulation</h2>\n    <p>\n      The interesting experimental punchline is what shows up in higher-order OTOCs: many-body interference that behaves a bit like an interferometer built out of a whole interacting quantum system.\n      That interference can amplify the measured quantum signal and partially undo the chaotic spreading, which changes how the signal decays over time.\n    </p>\n    <p>\n      In Google’s reported results, the OTOC signal’s characteristic magnitude decays as a power law (rather than exponentially in time), and that slower decay is one of the ingredients that helps push the task into a beyond-classical regime for the benchmark circuits they study on the Willow chip.\n    </p>\n\n    <h2>From Chaos to Measurements</h2>\n    <p>\n      What makes this more than a physics curiosity is the connection to Hamiltonian learning: if a quantum computer can efficiently generate OTOC signals for candidate models, those signals can be compared with experimental data to tune the model parameters.\n      This ties “quantum chaos diagnostics” to real measurement pipelines, like those found in spectroscopy.\n    </p>\n    <p>\n      Google highlights nuclear magnetic resonance (NMR) as a motivating domain for this kind of approach, because NMR experiments naturally produce time-dependent signals that can be related to underlying Hamiltonians.\n      Even when early demonstrations are still within classical reach, this mapping from lab data to learnable models is the important conceptual bridge.\n    </p>\n\n    <p><strong>Contributor:</strong> Alessandro Linzi</p>\n\n<p>\n      Read more here: <a href=\"https://research.google/blog/a-verifiable-quantum-advantage/\">Google Research Blog</a>\n    </p>\n  "
    },
    {
      "title": "Project Suncatcher: Pioneering Space-Based AI Infrastructure",
      "slug": "project-suncatcher-pioneering-space-based-ai-infrastructure",
      "publishedAt": "2025-11-27T09:55:00+01:00",
      "summary": "Project Suncatcher is a research moonshot exploring solar-powered satellite constellations equipped with TPUs and ultra-fast optical links to one day scale AI compute in space.",
      "tags": [
        "AI",
        "Space",
        "Infrastructure"
      ],
      "content": "\n    <p>\n      A question keeps coming up as AI scales: where do we find the energy and infrastructure to keep pushing compute forward without squeezing Earth’s resources even harder? Project Suncatcher is one of the boldest answers on the table—a research moonshot that explores whether serious machine learning compute could eventually live in orbit, powered directly by near-continuous sunlight.\n    </p>\n\n    <h2>Harnessing Solar Power for AI</h2>\n\n    <p>\n      The basic premise is straightforward: the Sun delivers an absurd amount of energy, and in the right orbit satellites can stay in sunlight for most (or nearly all) of their operational time. Project Suncatcher imagines compact constellations of solar-powered satellites, equipped with TPUs, designed to behave like a distributed “data center” in space while relying far less on terrestrial constraints like land and water.\n    </p>\n\n    <h2>Overcoming Technical Challenges</h2>\n\n    <p>\n      Scaling compute in space isn’t about launching one big box—it’s about making many satellites act like one cohesive system. The project highlights several core challenges that need to be solved before this idea can move from speculative to practical:\n    </p>\n\n    <ol>\n      <li>\n        High-bandwidth inter-satellite links: To run distributed ML workloads, the satellites would need data center-scale connectivity, targeting tens of terabits per second per link using approaches like dense wavelength-division multiplexing (DWDM) and spatial multiplexing.\n      </li>\n      <li>\n        Satellite formation control: Achieving those optical link budgets requires satellites flying in relatively tight and stable formations, with models that account for gravitational and orbital dynamics.\n      </li>\n      <li>\n        Radiation tolerance: Compute hardware in orbit must handle radiation exposure; the work discusses testing that suggests TPU designs can be more resilient than many people assume.\n      </li>\n      <li>\n        Economic viability: Even if the engineering works, launch costs have to fall far enough for orbital compute to compete with terrestrial buildouts, with projections pointing to improvements by the mid-2030s.\n      </li>\n    </ol>\n\n    <h2>Looking Ahead</h2>\n\n    <p>\n      What makes this feel more than a thought experiment is the plan to validate pieces of the stack in orbit. A learning mission in partnership with Planet is planned to launch prototype satellites by early 2027 to test hardware and key assumptions in the real environment.\n    </p>\n\n    <p>\n      The reason Project Suncatcher is worth paying attention to is not because “AI data centers in space” is guaranteed to happen, but because it forces the right systems-level conversation: energy, networking, reliability, and the physics of scaling. If the next decade is about expanding compute responsibly, exploring extreme options like this helps map the boundary of what’s possible.\n    </p>\n\n    <p><strong>Contributor:</strong> Alessandro Linzi</p>\n\n<p>\n      Read more here: <a href=\"https://research.google/blog/exploring-a-space-based-scalable-ai-infrastructure-system-design/\">Google Research Blog</a>\n    </p>\n  "
    },
    {
      "title": "Unraveling Consciousness: The Urgent Scientific Quest in the Age of AI",
      "slug": "unraveling-consciousness-urgent-scientific-quest-age-of-ai",
      "publishedAt": "2025-11-21T10:30:00+01:00",
      "summary": "Researchers argue consciousness science has become urgent as AI and neurotechnology accelerate, with major implications for medicine, ethics, law, and how society defines moral responsibility.",
      "tags": [
        "AI",
        "Neuroscience",
        "Ethics"
      ],
      "content": "\n            <p>\n                As artificial intelligence (AI) continues to evolve at a breathtaking pace, it brings with it not just technological marvels but profound ethical questions. Among the most pressing of these is the quest to understand human consciousness—a mystery that has intrigued philosophers and scientists for centuries. Now, researchers argue that this pursuit is more urgent than ever, as advances in AI and neurotechnology outstrip our grasp of what it means to be conscious.\n            </p>\n\n            <h2>The Scientific Imperative</h2>\n\n            <p>\n                In an October 2025 article published in Frontiers in Science, leading researchers Axel Cleeremans, Liad Mudrik, and Anil Seth highlight the rapid developments in AI and neurotechnologies like brain-computer interfaces. These advancements, they warn, could lead to the creation or detection of consciousness in machines or synthetic biological systems, posing significant ethical and existential risks.\n            </p>\n\n            <p>\n                Professor Axel Cleeremans from École Polytechnique de Bruxelles, and an ERC grantee, emphasizes that consciousness science has transcended philosophical debates, impacting every facet of society. \"Understanding consciousness is one of the most substantial challenges of 21st-century science—and it's now urgent due to advances in AI and other technologies,\" he states.\n            </p>\n\n            <h2>Far-Reaching Implications</h2>\n\n            <p>\n                The implications of cracking the consciousness code are vast:\n            </p>\n\n            <ul>\n                <li>\n                    Medical Advancements: Consciousness tests could revolutionize care for patients with brain injuries, potentially identifying awareness in those previously thought unconscious. This could transform treatment protocols and end-of-life decisions.\n                </li>\n                <li>\n                    Mental Health: A deeper understanding of subjective experience could bridge gaps between animal models and human emotions, leading to innovative therapies for conditions like depression and schizophrenia.\n                </li>\n                <li>\n                    Ethical Considerations: Determining consciousness in animals or AI would redefine moral responsibilities, influencing animal welfare laws, research practices, and even dietary choices.\n                </li>\n                <li>\n                    Legal Repercussions: Insights into conscious and unconscious decision-making processes could challenge legal concepts such as intent, necessitating a reevaluation of culpability.\n                </li>\n                <li>\n                    Neurotechnology Development: As AI and neurotechnologies advance, distinguishing between biological and artificial consciousness will be crucial, raising societal and ethical challenges.\n                </li>\n            </ul>\n\n            <h2>A Call for Collaborative Research</h2>\n\n            <p>\n                To address these challenges, the authors advocate for a coordinated, evidence-based approach to consciousness research. They propose adversarial collaborations, where competing theories are rigorously tested through joint experiments, to break theoretical silos and foster innovation.\n            </p>\n\n            <p>\n                Moreover, they stress the importance of incorporating phenomenology—the subjective experience of consciousness—into scientific studies, complementing functional analyses.\n            </p>\n\n            <h2>Preparing for the Future</h2>\n\n            <p>\n                As Professor Anil Seth of the University of Sussex notes, \"Progress in consciousness science will reshape how we see ourselves and our relationship to both artificial intelligence and the natural world.\" The potential to understand or even create consciousness demands proactive engagement from scientists, ethicists, policymakers, and the public to navigate the profound consequences that lie ahead.\n            </p>\n\n            <p>\n                In this era of rapid technological advancement, the quest to understand consciousness is not just a scientific endeavor—it is a societal imperative. By unraveling this mystery, we may not only gain insights into the nature of human experience but also forge a future that respects the complexities of consciousness in all its forms.\n            </p><p><strong>Contributor:</strong> Alessandro Linzi</p>\n\n            <p>\n                For further reading, visit the full article published in Frontiers in Science: <a href=\"https://www.frontiersin.org/journals/science/articles/10.3389/fsci.2025.1546279/full\">Consciousness science: where are we, where are we going, and what if we get there?</a>\n            </p>"
    },
    {
      "title": "Ax-Prover: Revolutionizing Automated Theorem Proving with Multi-Agent Systems",
      "slug": "ax-prover-revolutionizing-automated-theorem-proving-multi-agent-systems",
      "publishedAt": "2025-11-24T11:25:00+01:00",
      "summary": "Ax-Prover is a multi-agent system that combines LLM reasoning with Lean verification via MCP, delivering formally validated proofs and strong results across math and quantum-physics benchmarks.",
      "tags": [
        "AI",
        "Mathematics",
        "Quantum Physics"
      ],
      "content": "\n            <p>\n                In the ever-evolving landscape of artificial intelligence, Ax-Prover emerges as a groundbreaking multi-agent system designed to automate theorem proving in mathematics and quantum physics. By seamlessly integrating the creative reasoning capabilities of large language models (LLMs) with the formal verification rigor of the Lean proof assistant, Ax-Prover addresses longstanding challenges in automated reasoning and sets a new standard for efficiency and adaptability.\n            </p>\n\n            <h2>Architecture: A Symphony of Agents</h2>\n\n            <p>\n                At the heart of Ax-Prover lies a sophisticated architecture comprising three specialized agents: the Orchestrator, Prover, and Verifier. Coordinated through the Model Context Protocol (MCP), these agents engage in a closed-loop process of problem dispatch, iterative construction, and verification, ensuring the generation of formally validated Lean proofs.\n            </p>\n\n            <ul>\n                <li>\n                    Orchestrator: The maestro of this ensemble, the Orchestrator schedules proof tasks, distributes subtasks, manages feedback, and maintains the refinement loop. It orchestrates the collaborative efforts of the Prover and Verifier, ensuring that proofs are either verified or resources are optimally utilized.\n                </li>\n                <li>\n                    Prover: Leveraging the linguistic prowess of general-purpose LLMs, such as Claude Sonnet 4, the Prover synthesizes natural language proof sketches and incrementally translates them into Lean code. By utilizing Lean tools via MCP, the Prover enforces correctness through regular verification, bridging the gap between creative intuition and formal precision.\n                </li>\n                <li>\n                    Verifier: With a meticulous eye for detail, the Verifier operates on diagnostics from Lean to ensure that proofs are error-free and devoid of unproven placeholders. By collaborating closely with the Prover, the Verifier guarantees the integrity and reliability of the final proof.\n                </li>\n            </ul>\n\n            <h2>Benchmark Performance: Setting New Standards</h2>\n\n            <p>\n                Ax-Prover's prowess is evident in its impressive benchmark performance across various mathematical and scientific domains. Evaluated on both existing and newly created Lean benchmarks, Ax-Prover consistently outperforms specialized provers and achieves competitive results on established ones.\n            </p>\n\n            <ul>\n                <li>\n                    NuminaMath-LEAN: Ax-Prover achieves an overall accuracy of 51%, with a notable Pass@1 rate of 26% on unsolved problems, showcasing its ability to tackle complex mathematical challenges.\n                </li>\n                <li>\n                    Abstract Algebra AA: With an overall accuracy of 64%, Ax-Prover surpasses Mathlib LLMs, demonstrating its expertise in abstract algebraic structures.\n                </li>\n                <li>\n                    QuantumTheorems QT: Achieving a remarkable overall accuracy of 96%, Ax-Prover provides full coverage of easy problems and excels in quantum theory theorem proving.\n                </li>\n                <li>\n                    PutnamBench: Ranked third with an accuracy of 14%, Ax-Prover exhibits strong sample efficiency, outperforming other specialized provers using fewer compute resources.\n                </li>\n            </ul>\n\n            <h2>Generalization and Domain Adaptability</h2>\n\n            <p>\n                Unlike systems confined to narrow domains, Ax-Prover harnesses the broad-domain knowledge inherent in general-purpose LLMs. Through the MCP, it maintains up-to-date interaction with Lean libraries, enabling rapid adaptation to diverse disciplines such as algebra, quantum physics, and cryptography. This adaptability is further enhanced by its modular multi-agent framework, which supports component interchangeability and parallel development.\n            </p>\n\n            <h2>Practical Use Case: Cryptography Theorem Formalization</h2>\n\n            <p>\n                Ax-Prover's collaborative capabilities were put to the test in the formalization of a cryptography theorem related to branch number computation for non-singular matrices over finite fields. By co-structuring the proof, verifying lemmas, and error-checking intermediate steps, Ax-Prover assisted a human expert in completing the formalization on modest hardware within two working days. This practical use case underscores Ax-Prover's usability and potential to accelerate scientific research.\n            </p>\n\n            <h2>Future Directions: Towards a Learning Scientific Assistant</h2>\n\n            <p>\n                As Ax-Prover continues to evolve, ongoing development efforts focus on parallelization, long-term memory modules, and enhanced reasoning capabilities. These enhancements aim to transform Ax-Prover into a continually learning, memory-augmented scientific assistant capable of reliable reasoning across formalizable domains. By addressing known limitations of specialization and enabling rapid formalization in emerging fields, Ax-Prover paves the way for verifiable scientific artificial intelligence.\n            </p>\n\n            <p>\n                In conclusion, Ax-Prover represents a significant advancement in automated theorem proving, combining the strengths of LLMs and Lean to create a robust, adaptable, and collaborative framework. Its achievements in benchmark performance, domain adaptability, and practical applications position it as a cornerstone of modern scientific discovery, driving innovation and expanding the frontiers of knowledge.\n            </p>\n\n            <p><strong>Contributor:</strong> Alessandro Linzi</p>\n\n<p>\n                Read more here: <a href=\"https://www.emergentmind.com/topics/ax-prover\"> Emergent Mind</a>\n            </p>\n                    "
    },
    {
      "title": "Revolutionizing Life Sciences with Claude: From Discovery to Market",
      "slug": "revolutionizing-life-sciences-with-claude",
      "publishedAt": "2025-12-02T15:00:00+01:00",
      "summary": "Notes on Anthropic’s “Claude for Life Sciences”: better benchmark performance, connectors into core lab tools, and agent-like skills aimed at making research workflows less painful.",
      "tags": [
        "AI",
        "Life Sciences",
        "Anthropic"
      ],
      "content": "\n            <p>\n                I’ve been following how AI labs are trying to move from “chatting about science” to actually supporting scientific work end-to-end. Anthropic’s announcement of <em>Claude for Life Sciences</em> is an interesting step in that direction: the pitch is not just a smarter model, but a model plus integrations and task-focused capabilities that can plug into the messy reality of R&amp;D.\n            </p>\n            <p>\n                What caught my attention is the implied scope: from reading papers, to drafting protocols, to assisting with analysis, to helping with regulatory documentation. That’s ambitious, and it’s exactly where usefulness starts to matter more than vibes.\n            </p>\n\n            <h2>Enhanced Performance for Scientific Excellence</h2>\n            <p>\n                On the model side, the highlight is Claude Sonnet 4.5, positioned as their strongest model for life-sciences-flavored tasks. The announcement emphasizes improved performance on domain benchmarks (including Protocol QA and BixBench), with one concrete data point: Protocol QA at 0.83 versus a human baseline of 0.79.\n            </p>\n            <p>\n                If those numbers hold up in practice, the practical implication is simple: fewer “looks plausible but wrong” moments when the task is procedural (lab protocols, compliance steps, structured experimental reasoning). In life sciences, that reliability gap is often the difference between “nice demo” and “actually usable.”\n            </p>\n\n            <h2>Seamless Integration with Scientific Tools</h2>\n            <p>\n                The bigger story, to me, is the connector ecosystem. Instead of copying data back and forth between tools, Claude is meant to sit closer to where the work already happens. The announcement lists connectors for:\n            </p>\n            <ul>\n                <li><strong>Benchling</strong> (experimental data and records).</li>\n                <li><strong>BioRender</strong> (figures and templates).</li>\n                <li><strong>PubMed</strong> (biomedical literature).</li>\n                <li><strong>Scholar Gateway</strong> (peer-reviewed sources).</li>\n                <li><strong>Synapse.org</strong> (collaborative data sharing/analysis).</li>\n                <li><strong>10x Genomics</strong> (natural-language interaction for single-cell workflows).</li>\n            </ul>\n            <p>\n                Add the usual productivity integrations (Google Workspace, Microsoft Office), and the direction is clear: the model is being treated as a workflow layer, not just a text generator.\n            </p>\n\n            <h2>Empowering Researchers with Agent Skills</h2>\n            <p>\n                Another concept in the announcement is <em>Agent Skills</em>: packaged, repeatable task routines that Claude can run consistently. The initial focus mentioned is single-cell RNA sequencing quality control, borrowing best practices from the scverse ecosystem.\n            </p>\n            <p>\n                This matters because “do the same analysis every time, the same way” is exactly what many labs need—especially for routine QC and reporting. If researchers can also create custom skills, the platform shifts from “one model for everyone” to “local automation primitives” tailored to a lab’s workflow.\n            </p>\n\n            <h2>Comprehensive Support Across the Research Lifecycle</h2>\n            <p>\n                The announcement frames Claude as helpful across the whole lifecycle:\n            </p>\n            <ul>\n                <li><strong>Literature reviews &amp; hypothesis generation</strong>: summarizing biomedical papers and brainstorming directions.</li>\n                <li><strong>Protocol development</strong>: drafting study protocols and compliance docs (especially via Benchling).</li>\n                <li><strong>Data analysis</strong>: using Claude Code for processing genomic data and presenting results.</li>\n                <li><strong>Regulatory work</strong>: helping prepare and review submissions with fewer tedious iterations.</li>\n            </ul>\n            <p>\n                There’s also mention of a prompt library for common tasks. That’s a small detail, but it’s often what separates “power users can do magic” from “a normal team can adopt this without a month of trial-and-error.”\n            </p>\n\n            <h2>Partnerships and adoption</h2>\n            <p>\n                Anthropic also points to partnerships (Sanofi, Broad Institute, 10x Genomics) as a way to ground the product in real constraints. It’s hard to evaluate from the outside, but it signals they’re optimizing for actual deployment contexts rather than benchmark-only progress.\n            </p>\n            <p>\n                They also mention an “AI for Science” program with free API credits for impactful projects, which is a sensible way to seed experimentation in academia and early-stage research settings.\n            </p>\n\n            <h2>What I take from this</h2>\n            <p>\n                The takeaway is not “Claude will discover drugs autonomously tomorrow.” It’s that the industry is converging on a practical stack: (1) stronger reasoning, (2) tighter tool/data integration, and (3) repeatable task skills. If this works, it reduces friction in the boring-but-critical parts of research—where time disappears.\n            </p>\n            <p>\n                The open question is reliability under real lab conditions: edge cases, messy metadata, inconsistent protocols, and the human factors around validation. Still, the direction feels correct: make the model accountable to workflows, not just answers.\n            </p>\n\n            <p><strong>Contributor:</strong> Alessandro Linzi</p>\n\n<p>\n                Read more here: <a href=\"https://www.anthropic.com/news/claude-for-life-sciences\" target=\"_blank\" rel=\"noopener noreferrer\">Anthropic — Claude for Life Sciences</a>\n            </p>\n                    "
    },
    {
      "title": "Advancing Science and Math with GPT-5.2",
      "slug": "advancing-science-math-gpt-5-2",
      "publishedAt": "2025-12-13T12:45:00+01:00",
      "summary": "How GPT-5.2 Pro and Thinking are transforming scientific research with unprecedented mathematical reasoning and problem-solving capabilities.",
      "tags": [
        "AI",
        "Science",
        "Mathematics"
      ],
      "content": "\n            <h2>Stronger Performance Where Precision Matters</h2>\n            <p>\n                At the heart of GPT-5.2's prowess is its robust mathematical reasoning. This capability is crucial for scientific and technical work, where multi-step logic, consistent quantities, and error-free analyses are paramount. GPT-5.2's improvements on benchmarks like FrontierMath highlight its general reasoning and abstraction skills, which are essential for coding, data analysis, and experimental design.\n            </p>\n            <p>\n                On the GPQA Diamond benchmark, a graduate-level Q&A test covering physics, chemistry, and biology, GPT-5.2 Pro achieved an impressive 93.2% accuracy, with GPT-5.2 Thinking close behind at 92.4%. These results underscore the model's ability to handle complex scientific queries without external tools, relying solely on its reasoning capabilities.\n            </p>\n            <p>\n                In the field of mathematics, GPT-5.2 Thinking set a new record by solving 40.3% of expert-level problems on FrontierMath. This achievement demonstrates the model's capacity to engage with intricate mathematical concepts and provide solutions that were previously unattainable.\n            </p>\n            <h2>Case Study: Resolving Open Research Problems</h2>\n            <p>\n                GPT-5.2's impact extends beyond benchmark performance. In a notable case study, the model assisted researchers in resolving a long-standing open problem in statistical learning theory. The question at hand—whether more data consistently improves results in statistical models—has puzzled researchers for years. GPT-5.2 Pro provided a direct solution, which was then meticulously verified by human experts.\n            </p>\n            <p>\n                This collaboration highlights a new paradigm in scientific research, where AI models serve as tools for exploration and hypothesis testing, while human researchers ensure accuracy and interpret the findings. GPT-5.2's ability to extend its results to higher-dimensional settings further exemplifies its potential to drive innovation across various domains.\n            </p>\n            <h2>Looking Ahead: The Future of AI in Science</h2>\n            <p>\n                The advancements showcased by GPT-5.2 suggest a promising future for AI in scientific research. In fields with strong axiomatic foundations, such as mathematics and theoretical computer science, AI models can explore proofs, test hypotheses, and uncover connections that might otherwise require significant human effort.\n            </p>\n            <p>\n                However, it is essential to recognize that AI systems are not independent researchers. Expert judgment, verification, and domain understanding remain critical. By integrating AI into research workflows with a focus on validation, transparency, and collaboration, we can harness its full potential to accelerate scientific discovery.\n            </p>\n            <p>\n                In conclusion, GPT-5.2 represents a significant leap forward in AI's ability to assist and enhance scientific research. Its precision, reasoning capabilities, and collaborative potential make it an invaluable tool for researchers worldwide. As we continue to explore the possibilities of AI in science, GPT-5.2 stands as a testament to the transformative power of technology in advancing human knowledge.\n            </p>\n            <p><strong>Contributor:</strong> Alessandro Linzi</p>\n\n<p>\n                Read more here: <a href=\"https://openai.com/index/gpt-5-2-for-science-and-math/\"> OpenAI Blog</a>\n            </p>\n                    "
    }
  ]
}